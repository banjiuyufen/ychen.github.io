<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yi Chen - Academic Homepage</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&family=Merriweather:ital,wght@0,300;0,400;0,700;1,400&family=JetBrains+Mono:wght@400&display=swap" rel="stylesheet">
    <style>
        :root {
            /* 学术深蓝配色体系 */
            --primary: #003366; /* Oxford Blue */
            --primary-light: #2c5282;
            --primary-dark: #002244;
            --secondary: #536471; /* Slate */
            
            --bg-body: #fdfdfd;
            --bg-card: #ffffff;
            --border-color: #e1e4e8;
            
            --text-main: #24292f;
            --text-muted: #57606a;
            
            --accent: #d97706; /* Amber */
            
            --font-sans: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
            --font-serif: 'Merriweather', Georgia, 'Times New Roman', serif;
            --font-mono: 'JetBrains Mono', Consolas, monospace;

            --border-radius: 6px;
            --shadow-card: 0 1px 3px rgba(0,0,0,0.08), 0 4px 12px rgba(0,0,0,0.02);
            --shadow-hover: 0 8px 24px rgba(0,0,0,0.08);
            --transition: all 0.25s cubic-bezier(0.4, 0, 0.2, 1);
        }
        
        :root.dark-mode {
            --primary: #58a6ff;
            --primary-light: #79c0ff;
            --primary-dark: #1f6feb;
            --secondary: #8b949e;
            
            --bg-body: #0d1117;
            --bg-card: #161b22;
            --border-color: #30363d;
            
            --text-main: #c9d1d9;
            --text-muted: #8b949e;
            
            --accent: #d29922;
            
            --shadow-card: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-hover: 0 8px 24px rgba(0,0,0,0.4);
        }
        
        * { margin: 0; padding: 0; box-sizing: border-box; }
        
        body {
            font-family: var(--font-sans);
            line-height: 1.7;
            color: var(--text-main);
            background-color: var(--bg-body);
            background-image: radial-gradient(#e5e7eb 1px, transparent 1px);
            background-size: 20px 20px;
            padding: 40px 20px;
            min-height: 100vh;
            transition: background 0.3s ease, color 0.3s ease;
        }
        
        :root.dark-mode body { background-image: radial-gradient(#21262d 1px, transparent 1px); }

        .container { max-width: 1080px; margin: 0 auto; position: relative; z-index: 2; }
        
        .header { text-align: left; padding: 60px 0; margin-bottom: 20px; border-bottom: 1px solid var(--border-color); }
        .header h1 { font-family: var(--font-serif); font-size: 3.5rem; font-weight: 700; color: var(--text-main); margin-bottom: 10px; letter-spacing: -0.02em; }
        .header p { font-size: 1.25rem; color: var(--text-muted); font-weight: 300; max-width: 800px; }

        .grid { display: grid; gap: 30px; }
        .card { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: var(--border-radius); padding: 35px; box-shadow: var(--shadow-card); transition: var(--transition); }
        .card:hover { box-shadow: var(--shadow-hover); border-color: var(--primary-light); }
        
        .section-title { font-family: var(--font-sans); font-size: 1.5rem; font-weight: 600; color: var(--text-main); margin-bottom: 25px; padding-bottom: 12px; border-bottom: 2px solid var(--primary); display: flex; align-items: center; gap: 12px; letter-spacing: -0.01em; }
        .section-title i { font-size: 1.2rem; color: var(--primary); opacity: 0.8; }

        .profile { display: flex; gap: 40px; align-items: flex-start; }
        .profile-img { width: 180px; height: 180px; border-radius: 50%; object-fit: cover; border: 3px solid var(--bg-card); box-shadow: 0 4px 10px rgba(0,0,0,0.1); flex-shrink: 0; }
        
        a { color: var(--primary); text-decoration: none; transition: color 0.2s; }
        a:hover { text-decoration: underline; color: var(--primary-light); }

        .social-links { display: flex; flex-wrap: wrap; gap: 12px; margin: 20px 0; }
        .social-link { font-size: 0.9rem; padding: 6px 14px; border-radius: 4px; background: transparent; border: 1px solid var(--border-color); color: var(--text-muted); display: flex; align-items: center; gap: 8px; }
        .social-link:hover { background: var(--primary); color: #fff; border-color: var(--primary); text-decoration: none; }

        .contact-info { font-size: 0.95rem; color: var(--text-muted); display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 10px; margin-top: 20px; padding-top: 20px; border-top: 1px dashed var(--border-color); }
        .contact-row { display: flex; align-items: center; gap: 10px; }
        .contact-row i { color: var(--primary); width: 16px; }

        .research-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 30px; }
        .research-interests li { margin-bottom: 10px; list-style-type: none; padding-left: 20px; position: relative; }
        .research-interests li::before { content: "▹"; position: absolute; left: 0; color: var(--primary); }

        .highlight { color: var(--accent); font-weight: 600; background: rgba(217, 119, 6, 0.1); padding: 0 4px; border-radius: 3px; }
        .note { margin-top: 20px; padding: 15px; background: rgba(0, 51, 102, 0.04); border-left: 3px solid var(--primary); font-size: 0.95rem; font-style: italic; color: var(--text-muted); }
        :root.dark-mode .note { background: rgba(88, 166, 255, 0.1); }

        .publication-list { list-style: none; }
        .publication-list li { margin-bottom: 30px; position: relative; padding-left: 20px; }
        
        .publication-title { 
            font-family: var(--font-serif); 
            font-size: 1.15rem; 
            font-weight: 700; 
            color: var(--text-main); 
            margin-bottom: 6px; 
            cursor: pointer; 
            line-height: 1.4; 
            position: relative; 
            display: inline-block;
            transition: color 0.2s;
        }
        .publication-title:hover { color: var(--primary); }
        /* Add a dotted underline to indicate interactivity */
        .publication-title::after {
            content: '';
            position: absolute;
            width: 100%;
            height: 2px;
            bottom: -2px;
            left: 0;
            background-image: linear-gradient(to right, var(--text-muted) 33%, rgba(255,255,255,0) 0%);
            background-position: bottom;
            background-size: 5px 1px;
            background-repeat: repeat-x;
            opacity: 0.5;
        }
        
        .publication-authors { font-size: 1rem; color: var(--text-muted); margin-bottom: 6px; }
        .publication-venue { font-size: 0.95rem; font-style: italic; color: var(--primary); margin-bottom: 10px; }
        .publication-links a { font-size: 0.85rem; font-family: var(--font-mono); text-transform: uppercase; margin-right: 15px; color: var(--text-muted); font-weight: 600; }
        .publication-links a:hover { color: var(--primary); }

        .academic-activities { display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 20px; }
        .activity-item, .contribution-item { background: var(--bg-body); border: 1px solid var(--border-color); border-radius: var(--border-radius); padding: 20px; transition: var(--transition); }
        .activity-item:hover, .contribution-item:hover { transform: translateY(-3px); border-color: var(--primary); }
        .activity-title, .contribution-title { font-weight: 600; font-size: 1.05rem; margin-bottom: 8px; display: flex; justify-content: space-between; align-items: flex-start; }
        .activity-year { font-family: var(--font-mono); font-size: 0.8rem; background: var(--border-color); padding: 2px 8px; border-radius: 4px; color: var(--text-main); }

        .tech-stack { margin-top: 15px; display: flex; gap: 8px; flex-wrap: wrap; }
        .tech-item { font-size: 0.75rem; font-family: var(--font-mono); background: rgba(0,0,0,0.05); padding: 2px 8px; border-radius: 3px; color: var(--text-muted); }
        :root.dark-mode .tech-item { background: rgba(255,255,255,0.1); }

        .control-bar { position: absolute; top: 60px; right: 0; display: flex; gap: 10px; }
        .control-btn { background: transparent; border: 1px solid var(--border-color); color: var(--text-muted); width: 36px; height: 36px; border-radius: 50%; cursor: pointer; display: flex; align-items: center; justify-content: center; transition: all 0.2s; }
        .control-btn:hover { background: var(--bg-card); color: var(--primary); border-color: var(--primary); box-shadow: var(--shadow-card); }

        .footer { text-align: center; margin-top: 60px; padding-top: 30px; border-top: 1px solid var(--border-color); font-size: 0.9rem; color: var(--text-muted); }

        /* --- RE-IMPLEMENTED TOOLTIP STYLES --- */
        .tooltip-container {
            display: none; /* Hidden by default */
            position: fixed; /* Fixed relative to viewport */
            background: var(--bg-card);
            border: 1px solid var(--border-color);
            padding: 20px 25px;
            width: 600px;
            max-width: 90vw;
            border-radius: var(--border-radius);
            box-shadow: 0 10px 40px rgba(0,0,0,0.2);
            z-index: 10000; /* Extremely high Z-Index */
            font-size: 0.95rem;
            line-height: 1.6;
            text-align: justify;
            color: var(--text-main);
            pointer-events: none; /* Prevent tooltip from blocking mouse events */
        }
        
        .tooltip-container.visible {
            display: block;
            animation: fadeIn 0.2s ease-out;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(5px); }
            to { opacity: 1; transform: translateY(0); }
        }
        
        :root.dark-mode .tooltip-container {
            box-shadow: 0 10px 40px rgba(0,0,0,0.6);
            border: 1px solid var(--border-color);
        }

        .tooltip-label {
            font-family: var(--font-mono);
            font-size: 0.75rem;
            text-transform: uppercase;
            color: var(--primary);
            margin-bottom: 8px;
            display: block;
            font-weight: 600;
            letter-spacing: 0.5px;
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 4px;
        }

        @media (max-width: 768px) {
            .profile { flex-direction: column; align-items: center; text-align: center; }
            .header h1 { font-size: 2.5rem; }
            .research-grid { grid-template-columns: 1fr; }
            .contact-info { justify-content: center; }
            .control-bar { position: relative; top: 0; justify-content: flex-end; margin-bottom: 20px; }
            .header { padding-top: 20px; }
            .tooltip-container { display: none !important; } /* Disable on mobile */
        }
    </style>
</head>
<body>
    
    <div id="abstract-tooltip" class="tooltip-container">
        <span class="tooltip-label">Abstract</span>
        <div id="tooltip-content"></div>
    </div>

    <div class="container">
        <div class="control-bar">
            <button class="control-btn theme-toggle" id="themeToggle" title="Switch Theme">
                <i class="fas fa-moon"></i>
            </button>
            <button class="control-btn lang-toggle" id="langToggle" title="Switch Language">
                <span style="font-size: 0.8rem; font-weight: bold;">EN</span>
            </button>
        </div>

        <div class="header">
            <h1>Yi Chen</h1>
            <p class="lang-switch" data-lang-en="PhD Candidate in Pattern Recognition and Intelligent Systems" data-lang-zh="模式识别与智能系统专业 · 博士候选人">
                PhD Candidate in Pattern Recognition and Intelligent Systems
            </p>
        </div>
        
        <div class="grid">
            <div class="card">
                <div class="profile">
                    <img src="ychen.jpg" alt="Yi Chen" class="profile-img">
                    <div class="profile-info">
                        <h2 class="section-title">
                            <i class="fas fa-user-circle"></i>
                            <span class="lang-switch" data-lang-en="Personal Profile" data-lang-zh="个人简介">Personal Profile</span>
                        </h2>
                        <p class="lang-switch" data-lang-en="I am currently a PhD student jointly affiliated with the State Key Laboratory of Multimodal Artificial Intelligence Systems at the Institute of Automation, Chinese Academy of Sciences (CASIA) and Zhongguancun Academy, under the supervision of Prof. <a href='https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN'>Cheng-Lin Liu</a>." data-lang-zh="本人目前是中国科学院自动化研究所多模态人工智能系统国家重点实验室（MAIS）与中关村学院联合培养的博士研究生，师从<a href='https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN'>刘成林</a>研究员。">
                            I am currently a PhD student jointly affiliated with the State Key Laboratory of Multimodal Artificial Intelligence Systems at the Institute of Automation, Chinese Academy of Sciences (CASIA) and Zhongguancun Academy, under the supervision of Prof. <a href="https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN">Cheng-Lin Liu</a>.
                        </p>
                        <p class="lang-switch" data-lang-en="I received my B.E. degree from the School of Space Science and Technology at Xidian University in 2021. Subsequently, I earned my Master's degree in Electronic Information from the National Laboratory of Pattern Recognition (NLPR), CASIA in 2024, also advised by Prof. <a href='https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN'>Cheng-Lin Liu</a>." data-lang-zh="本人于2021年在西安电子科技大学空间科学与技术学院获得工学学士学位；2024年在中国科学院自动化研究所模式识别国家重点实验室（NLPR）获得电子信息硕士学位，导师同样为<a href='https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN'>刘成林</a>研究员。">
                            I received my B.E. degree from the School of Space Science and Technology at Xidian University in 2021. Subsequently, I earned my Master's degree in Electronic Information from the National Laboratory of Pattern Recognition (NLPR), CASIA in 2024, also advised by Prof. <a href='https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN'>Cheng-Lin Liu</a>.
                        </p>
                        
                        <div class="social-links">
                            <a href="https://scholar.google.com/citations?user=XoiT9wMAAAAJ&hl=zh-CN" class="social-link"><i class="fas fa-graduation-cap"></i> Google Scholar</a>
                            <a href="https://orcid.org/0009-0005-0720-6372" class="social-link"><i class="fab fa-orcid"></i> ORCID</a>
                            <a href="https://www.researchgate.net/profile/Yi-Chen-287" class="social-link"><i class="fab fa-researchgate"></i> ResearchGate</a>
                            <a href="https://github.com/banjiuyufen" class="social-link"><i class="fab fa-github"></i> GitHub</a>
                        </div>
                        
                        <div class="contact-info">
                            <div class="contact-row">
                                <i class="fas fa-building"></i>
                                <span><a href="https://mais.ia.ac.cn/about/ds.html">CASIA-MAIS / NLPR-PAL Group</a></span>
                            </div>
                            <div class="contact-row">
                                <i class="fas fa-map-marker-alt"></i>
                                <span>Beijing 100190, China</span>
                            </div>
                            <div class="contact-row">
                                <i class="fas fa-envelope"></i>
                                <span>yi.chen@nlpr.ia.ac.cn</span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="research-grid">
                <div class="card">
                    <h2 class="section-title">
                        <i class="fas fa-microscope"></i>
                        <span class="lang-switch" data-lang-en="Research Focus" data-lang-zh="研究方向">Research Focus</span>
                    </h2>
                    <p class="lang-switch" data-lang-en="My Ph.D. research focuses on artificial intelligence and deep learning methodologies, specifically at the intersection of Large Language Models (LLMs) and Adjuvant Science (<span class='highlight'>AI for Adjuvant Discovery</span>). Additionally, I investigate the fundamental theories of Multimodal Large Language Models (MLLMs), including reliable reasoning and inference acceleration." data-lang-zh="本人的博士研究课题专注于人工智能与深度学习方法论，特别是大型语言模型与佐剂学的交叉领域研究（<span class='highlight'>AI佐剂发现</span>）。此外，本人亦致力于多模态大模型（MLLMs）的基础理论研究，涵盖可靠推理与推理加速等前沿方向。">
                        My Ph.D. research focuses on artificial intelligence and deep learning methodologies, specifically at the intersection of Large Language Models (LLMs) and Adjuvant Science (<span class="highlight">AI for Adjuvant Discovery</span>). Additionally, I investigate the fundamental theories of Multimodal Large Language Models (MLLMs), including reliable reasoning and inference acceleration.
                    </p>
                    
                    <div class="note">
                        <span class="lang-switch" data-lang-en="Bridging AI theory with practical applications in scientific domains." data-lang-zh="致力于将人工智能理论与科学领域的实际应用相结合，推动AI for Science的发展。">
                            Bridging AI theory with practical applications in scientific domains.
                        </span>
                    </div>
                </div>
                
                <div class="card">
                    <h2 class="section-title">
                        <i class="fas fa-star"></i>
                        <span class="lang-switch" data-lang-en="Research Interests" data-lang-zh="研究兴趣">Research Interests</span>
                    </h2>
                    <ul class="research-interests">
                        <li class="lang-switch" data-lang-en="Recognition and generation of Online Handwritten Chinese Text" data-lang-zh="联机中文手写文本行的识别与生成">
                            Recognition and generation of Online Handwritten Chinese Text
                        </li>
                        <li class="lang-switch" data-lang-en="Foundations and applications of LLMs and MLLMs" data-lang-zh="大语言模型与多模态基座模型的基础理论与应用">
                            Foundations and applications of LLMs and MLLMs
                        </li>
                        <li class="lang-switch" data-lang-en="AI for Science (Biology, Chemistry, Materials Science)" data-lang-zh="AI for Science（生物、化学及材料科学方向）">
                            AI for Science (Biology, Chemistry, Materials Science)
                        </li>
                    </ul>
                    <div style="margin-top: 20px; font-size: 0.9rem; color: var(--primary);">
                         <span class="lang-switch" data-lang-en="Open to collaboration. Please feel free to contact via email." data-lang-zh="诚挚欢迎学术交流与合作，请通过邮件联系。">
                            Open to collaboration. Please feel free to contact via email.
                        </span>
                    </div>
                </div>
            </div>
            
            <div class="card">
                <h2 class="section-title">
                    <i class="fas fa-code-branch"></i>
                    <span class="lang-switch" data-lang-en="Community Contributions" data-lang-zh="开源与社区贡献">Community Contributions</span>
                </h2>
                
                <div class="contribution-item">
                    <h3 class="contribution-title">
                        <span class="lang-switch" data-lang-en="PaddleScience Contributor" data-lang-zh="PaddleScience 核心贡献">PaddleScience Contributor</span>
                        <a href="https://github.com/PaddlePaddle/PaddleScience/pull/977" style="font-size: 0.9rem;"><i class="fas fa-link"></i> PR #977</a>
                    </h3>
                    
                    <p class="lang-switch" data-lang-en="Integrated the <strong><a href='https://doi.org/10.1103/PhysRevLett.120.145301'>Crystal Graph CNN (CGCNN)</a></strong> model into the <strong>PaddleScience</strong> toolkit for materials chemistry applications." data-lang-zh="成功将 <strong><a href='https://doi.org/10.1103/PhysRevLett.120.145301'>Crystal Graph CNN (CGCNN)</a></strong> 模型集成至百度飞桨科学计算工具包 <strong>PaddleScience</strong> 中，用于材料化学领域的应用。">
                        Integrated the <strong><a href='https://doi.org/10.1103/PhysRevLett.120.145301'>Crystal Graph CNN (CGCNN)</a></strong> model into the <strong>PaddleScience</strong> toolkit for materials chemistry applications.
                    </p>
                    
                    <ul class="research-interests" style="margin-top: 10px; font-size: 0.95rem;">
                        <li class="lang-switch" data-lang-en="Implemented full pipeline: preprocessing, graph construction, training, and inference." data-lang-zh="完整实现了晶体结构数据预处理、图神经网络构建、训练及推理的全流程。">
                            Implemented full pipeline: preprocessing, graph construction, training, and inference.
                        </li>
                        <li class="lang-switch" data-lang-en="Code merged into official repository and featured as an official case study." data-lang-zh="代码已合并至官方主仓库，并被收录为官方材料化学应用案例。">
                            Code merged into official repository and featured as an official case study.
                        </li>
                    </ul>
                    
                    <div class="tech-stack">
                        <span class="tech-item">PaddlePaddle</span>
                        <span class="tech-item">GNN</span>
                        <span class="tech-item">Materials Informatics</span>
                    </div>
                </div>
            </div>
            
            <div class="card">
                <h2 class="section-title">
                    <i class="fas fa-user-tie"></i>
                    <span class="lang-switch" data-lang-en="Academic Service" data-lang-zh="学术服务">Academic Service</span>
                </h2>
                
                <div class="academic-activities">
                    <div class="activity-item">
                        <div class="activity-title">
                            AAAI 2026
                            <span class="activity-year">PC Member</span>
                        </div>
                        <div class="lang-switch" data-lang-en="Program Committee Member (Main Track)" data-lang-zh="主会程序委员会委员">
                            Program Committee Member (Main Track)
                        </div>
                    </div>

                    <div class="activity-item">
                        <div class="activity-title">
                            ICLR 2026
                            <span class="activity-year">Reviewer</span>
                        </div>
                        <div class="lang-switch" data-lang-en="Main Track Reviewer" data-lang-zh="主会审稿人">
                            Main Track Reviewer
                        </div>
                    </div>

                    <div class="activity-item">
                        <div class="activity-title">
                            CVPR 2026
                            <span class="activity-year">Reviewer</span>
                        </div>
                        <div class="lang-switch" data-lang-en="Main Track Reviewer" data-lang-zh="主会审稿人">
                            Main Track Reviewer
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="card">
                <h2 class="section-title">
                    <i class="fas fa-file-contract"></i>
                    <span class="lang-switch" data-lang-en="Preprints" data-lang-zh="预印本论文">Preprints</span>
                </h2>
                <p style="font-size: 0.9rem; color: var(--text-muted); margin-bottom: 20px;">
                    <i class="lang-switch" data-lang-en="(*: equal contribution)" data-lang-zh="(*: 同等贡献)">(*: equal contribution)</i>
                </p>
                
                <ul class="publication-list">
                    <li>
                        <div class="publication-title lang-switch" 
                             data-lang-en="An Open-Ended Benchmark and Formal Framework for Adjuvant Research with MLLM" 
                             data-lang-zh="面向大模型佐剂研究的开放式基准测试与形式化框架"
                             data-abstract-en="Adjuvants play a critical role in modulating immune responses and are central to the development of vaccines and immunotherapies. Yet progress in this field is constrained by data scarcity and incomplete understanding of mechanisms of action, which limit the transition from experience-based design to AI-driven approaches. To address these challenges, we present the first benchmark dedicated to adjuvants, constructed in an open-ended Q&A format and annotated by domain experts. The benchmark comprises 1,294 Q&A pairs and 1,364 formal descriptions, providing a resource for evaluating general-purpose multimodal large language models (MLLMs) and for developing domain-specific systems."
                             data-abstract-zh="佐剂在调节免疫反应中起着至关重要的作用，是疫苗和免疫疗法开发的核心。然而，由于数据匮乏以及对作用机制理解的不完整，该领域的进展受到限制，阻碍了从基于经验的设计向AI驱动方法的转变。鉴于此，我们提出了首个专门针对佐剂的开放式基准测试，该数据集由领域专家进行标注，包含1,294对问答和1,364条形式化描述，为评估通用多模态大语言模型（MLLMs）和开发领域特定系统提供了重要资源。">
                            An Open-Ended Benchmark and Formal Framework for Adjuvant Research with MLLM
                        </div>
                        <div class="publication-authors"><span class="highlight">Yi Chen*</span>, Yu Zhang*, Jian Xu, Xu-Yao Zhang, Hua Yue, Xinming Wang, Zequan Lyu, Wei Wei, Cheng-Lin Liu</div>
                        <div class="publication-links">
                            <a href="#"><i class="fas fa-file-pdf"></i> arXiv (Coming Soon)</a>
                        </div>
                    </li>
                    
                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="Sparsity Meets Similarity: Leveraging Long-Tail Distribution for Dynamic Optimized Token Representation in Multimodal Large Language Models"
                             data-lang-zh="稀疏性遇见相似性：利用长尾分布优化多模态大模型的动态Token表示"
                             data-abstract-en="Recently, multimodal large language models (MM-LLMs) have achieved significant success in various tasks, but their high computational costs limit widespread application. The main computational burden arises from processing concatenated text and visual tokens in the LLM layer. We propose a dynamic pruning algorithm that identifies the inflection point in the visual CLS token similarity curve, enabling effective trimming of visual markers to accelerate model performance."
                             data-abstract-zh="近年来，多模态大语言模型（MM-LLMs）在各项任务中取得了显著成功，但高昂的计算成本限制了其广泛应用。主要的计算负担源于LLM层对拼接文本和视觉Token的处理。为此，我们提出了一种动态剪枝算法，通过识别视觉CLS Token相似度曲线中的拐点，有效修剪冗余视觉标记以提升模型推理效率。">
                            Sparsity Meets Similarity: Leveraging Long-Tail Distribution for Dynamic Optimized Token Representation in Multimodal Large Language Models
                        </div>
                        <div class="publication-authors"><span class="highlight">Yi Chen*</span>, Gao-Tong Yu*, Jian Xu</div>
                        <div class="publication-links">
                            <a href="https://arxiv.org/abs/2409.01162"><i class="fas fa-file-pdf"></i> arXiv</a>
                        </div>
                    </li>

                    <li>
                         <div class="publication-title lang-switch"
                              data-lang-en="An Efficient Strategy for Data-constrained Machine Learning in Materials Science"
                              data-lang-zh="材料科学中数据受限场景下的高效机器学习策略"
                              data-abstract-en="Materials science research increasingly benefits from the application of machine learning method, yet encounters fundamental challenges from data scarcity. We develop a multi-task and auxiliary machine learning framework to address these limitations. Using the 2D materials dataset as a case study, our approach demonstrates significantly enhanced prediction accuracy over the baseline crystal graph convolutional neural networks method."
                              data-abstract-zh="材料科学研究日益受益于机器学习方法的应用，但仍面临数据匮乏带来的根本性挑战。为此，我们开发了一种多任务辅助机器学习框架以应对这些限制。以二维材料数据集为例，我们的方法在预测精度上显著优于基线晶体图卷积神经网络方法。">
                            An Efficient Strategy for Data-constrained Machine Learning in Materials Science
                        </div>
                        <div class="publication-authors">ChunTing Shao*, <span class="highlight">Yi Chen*</span>, ShanMan Song, PeiPei Yang, QingBo Yan, Jian Xu, Gang Su</div>
                        <div class="publication-links">
                             <a href="#"><i class="fas fa-file-pdf"></i> arXiv (Coming Soon)</a>
                        </div>
                    </li>

                    <li>
                         <div class="publication-title lang-switch"
                              data-lang-en="ContextRGBNav: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation"
                              data-lang-zh="ContextRGBNav：基于上下文自适应的单目零样本语义导航框架"
                              data-abstract-en="Efficiently finding objects in complex environments is fundamental to real-world embodied applications. We propose ContextRGBNav, a novel zero-shot, open-vocabulary semantic navigation framework that operates using only a monocular camera. Leveraging powerful 3D foundation models, ContextRGBNav eliminates the dependency on depth and pose while exhibiting strong ICL capability."
                              data-abstract-zh="在复杂环境中高效寻找物体是现实世界具身智能应用的基础。我们提出了ContextRGBNav，一种新颖的零样本、开放词汇语义导航框架，仅依赖单目相机运行。利用强大的3D基础模型，ContextRGBNav消除了对深度和姿态信息的依赖，同时展现出强大的上下文学习（ICL）能力。">
                            ContextRGBNav: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation
                        </div>
                        <div class="publication-authors">Ming-Ming Yu, <span class="highlight">Yi Chen</span>, Börje F. Karlsson, Wenjun Wu</div>
                        <div class="publication-links">
                             <a href="#"><i class="fas fa-file-pdf"></i> arXiv (Coming Soon)</a>
                        </div>
                    </li>

                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning"
                             data-lang-zh="ChartAgent：一种基于工具集成推理的图表理解框架"
                             data-abstract-en="Although recent multimodal large language models (MLLMs) have achieved notable advances in automated chart understanding, they still exhibit a pronounced dependence on explicit textual annotations. To overcome this limitation, we propose ChartAgent, a chart understanding framework built upon Tool Integrated Reasoning (TIR). Inspired by human cognitive processes, ChartAgent decomposes complex chart-analysis tasks into a sequence of observable and replayable steps."
                             data-abstract-zh="尽管多模态大语言模型（MLLMs）在图表自动理解方面取得了显著进展，但仍明显依赖显式的文本标注。为克服这一局限，我们提出了ChartAgent，一种基于工具集成推理（TIR）的图表理解框架。受人类认知过程启发，ChartAgent将复杂的图表分析任务分解为一系列可观察且可复现的步骤。">
                           ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning
                       </div>
                       <div class="publication-authors">Boran Wang, Xinming Wang, <span class="highlight">Yi Chen</span>, Xiang Li, Jian Xu, Jing Yuan, Cheng-Lin Liu</div>
                       <div class="publication-links">
                            <a href="#"><i class="fas fa-file-pdf"></i> arXiv (Coming Soon)</a>
                       </div>
                   </li>

                   <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="MR-ALIGN: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models"
                             data-lang-zh="MR-ALIGN：基于元推理信息的推理大模型事实性对齐"
                             data-abstract-en="Large reasoning models (LRMs) show strong capabilities in complex reasoning, yet their marginal gains on evidence-dependent factual questions are limited. We propose MR-ALIGN, a Meta-Reasoning informed alignment framework that enhances factuality without relying on external verifiers. MR-ALIGN quantifies state-transition probabilities along the model’s thinking process and constructs a transition-aware implicit reward."
                             data-abstract-zh="大推理模型（LRMs）在复杂推理任务中表现出强大的能力，但在依赖证据的事实性问题上增益有限。我们提出了MR-ALIGN，一种基于元推理信息的对齐框架，无需依赖外部验证器即可增强事实性。MR-ALIGN量化了模型思维过程中的状态转移概率，并构建了感知转移的隐式奖励。">
                           MR-ALIGN: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models
                       </div>
                       <div class="publication-authors">Xinming Wang, Jian Xu, Bin Yu, Sheng Lian, Hongzhu Yi, <span class="highlight">Yi Chen</span>, Boran Wang, Haoran Du, Han Hu, Xuyao Zhang, Chenglin Liu</div>
                       <div class="publication-links">
                            <a href="#"><i class="fas fa-file-pdf"></i> arXiv (Coming Soon)</a>
                       </div>
                   </li>

                   <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="One Patch Doesn’t Fit All: Adaptive Patching for Native-Resolution Multimodal Large Language Models"
                             data-lang-zh="One Patch Doesn’t Fit All：面向原生分辨率多模态大模型的自适应Patching策略"
                             data-abstract-en="Real-world visual signals are inherently variable in resolution. We argue that the rigid use of a single patch size is the primary cause of performance degradation. To address this issue, we introduce Adaptive Patching (AdaPatch), a simple yet effective strategy that adjusts patch size according to image resolution and information density and could be seamlessly plugged into pre-trained fixed-patch MLLMs."
                             data-abstract-zh="现实世界的视觉信号在分辨率上具有内在的多变性。我们认为，僵化地使用单一尺寸的Patch是导致性能下降的主要原因。为解决这一问题，我们引入了自适应Patching（AdaPatch），这是一种简单而有效的策略，能够根据图像分辨率和信息密度调整Patch大小，并可无缝集成到预训练的固定Patch MLLMs中。">
                           One Patch Doesn’t Fit All: Adaptive Patching for Native-Resolution Multimodal Large Language Models
                       </div>
                       <div class="publication-authors">Wenzhuo Liu, Weijie Yin, Fei Zhu, Shijie Ma, Haiyang Guo, <span class="highlight">Yi Chen</span>, Xiao-Hui Li, LiangXiao, ChaoFeng, Cheng-Lin Liu</div>
                       <div class="publication-links">
                            <a href="#"><i class="fas fa-file-pdf"></i> arXiv (Coming Soon)</a>
                       </div>
                   </li>

                   <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="The Evolution of Video Anomaly Detection: A Unified Framework from DNN to MLLM"
                             data-lang-zh="视频异常检测的演进：从DNN到MLLM的统一框架"
                             data-abstract-en="Video anomaly detection (VAD) aims to identify and ground anomalous behaviors or events in videos. This paper presents the first comprehensive survey analyzing VAD methods based on MLLMs and LLMs, providing an in-depth discussion of the changes occurring in the VAD field in the era of large models and their underlying causes."
                             data-abstract-zh="视频异常检测（VAD）旨在识别和定位视频中的异常行为或事件。本文首次全面综述了基于MLLMs和LLMs的VAD方法，深入探讨了大模型时代VAD领域发生的变革及其根本原因。">
                           The Evolution of Video Anomaly Detection: A Unified Framework from DNN to MLLM
                       </div>
                       <div class="publication-authors">Shibo Gao, Peipei Yang, Haiyang Guo, Yangyang Liu, <span class="highlight">Yi Chen</span>, Shuai Li, Han Zhu, Jian Xu, Xu-Yao Zhang, Linlin Huang</div>
                       <div class="publication-links">
                           <a href="https://arxiv.org/abs/2507.21649"><i class="fas fa-file-pdf"></i> arXiv</a>
                       </div>
                   </li>

                   <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction"
                             data-lang-zh="MeteorPred：面向极端天气预测的气象多模态大模型与数据集"
                             data-abstract-en="Timely and accurate severe weather alerts are critical for disaster mitigation. We introduce MP-Bench, the first large-scale temporal multimodal dataset for severe weather events prediction. On top of this dataset, we develop a meteorology multimodal large model (MMLM) that directly ingests 4D meteorological inputs."
                             data-abstract-zh="及时且准确的极端天气预警对于减灾至关重要。我们推出了MP-Bench，这是首个用于极端天气事件预测的大规模时序多模态数据集。在此基础上，我们开发了一种气象多模态大模型（MMLM），能够直接处理4D气象输入数据。">
                           MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction
                       </div>
                       <div class="publication-authors">Shuo Tang, Jian Xu, Jiadong Zhang, <span class="highlight">Yi Chen</span>, Qizhao Jin, Lingdong Shen, Cheng-Lin Liu, Shiming Xiang </div>
                       <div class="publication-links">
                           <a href="https://arxiv.org/abs/2508.06859"><i class="fas fa-file-pdf"></i> arXiv</a>
                       </div>
                   </li>

                   <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="The Hitchhiker's Guide to Scientific Agents: A Journey Through the Cosmos of Research Automation"
                             data-lang-zh="科学智能体银河指南：探索科研自动化之旅"
                             data-abstract-en="The advancement of LLM-based agents heralds a new perspective for AI for Science (AI4S). This survey is grounded in the standardized scientific research process and elucidates the construction and evaluation of scientific agents. We delineate the substantial distinctions between scientific agents and general-purpose agents and present a systematic taxonomy."
                             data-abstract-zh="基于大语言模型的智能体进展为AI for Science (AI4S) 带来了新的视角。本综述立足于标准化的科学研究流程，阐述了科学智能体的构建与评估。我们界定了科学智能体与通用智能体之间的实质性区别，并提出了系统的分类体系。">
                           The Hitchhiker's Guide to Scientific Agents: A Journey Through the Cosmos of Research Automation
                       </div>
                       <div class="publication-authors">Xinming Wang, Aslan Feng, Jian Xu, <span class="highlight">Yi Chen</span>, Haiyang Guo, Fei Zhu, Minsi Ren, Yuanqi Shao, Hongzhu Yi, Hongming Yang, Winston Hu, Tailin Wu, Xuyao Zhang, Cheng-Lin Liu</div>
                       <div class="publication-links">
                           <a href="https://www.techrxiv.org/users/951553/articles/1320864-the-hitchhiker-s-guide-to-autonomous-research-a-survey-of-scientific-agents"><i class="fas fa-file-pdf"></i> TechRxiv</a>
                           <a href="https://github.com/gudehhh666/Awesome_Scientific_Agent"><i class="fab fa-github"></i> GitHub</a>
                       </div>
                   </li>
                   
                   <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="Fine-Grained Post-Training Quantization for Large Vision Language Models with Integrated Gradients"
                             data-lang-zh="基于积分梯度的多模态大模型细粒度后训练量化方法"
                             data-abstract-en="Large Vision Language Models (LVLMs) come with a surge in computational overhead. Drawn from the concept of axiomatic attribution from mechanistic interpretability, we exploit integrated gradients to effectively evaluate the sensitivity, and push the granularity from modality to token to reflect both inter-modality and intra-modality dynamics."
                             data-abstract-zh="大型视觉语言模型（LVLMs）带来了激增的计算开销。借鉴机理可解释性中的公理归因概念，我们利用积分梯度有效评估敏感度，并将粒度从模态推向Token级别，以反映模态间和模态内的动态变化。">
                           Fine-Grained Post-Training Quantization for Large Vision Language Models with Integrated Gradients
                       </div>
                       <div class="publication-authors">Ziwen Xiang, Fanhu Zeng, Hongjian Fang, Rui-Qi Wang, Renxing Chen, <span class="highlight">Yi Chen</span>, Yanan Zhu, Peipei Yang, Xu-Yao Zhang</div>
                       <div class="publication-links">
                           <a href="https://arxiv.org/abs/2507.21649"><i class="fas fa-file-pdf"></i> arXiv</a>
                       </div>
                   </li>
                </ul>
            </div>
            
            <div class="card">
                <h2 class="section-title">
                    <i class="fas fa-book-journal-whills"></i>
                    <span class="lang-switch" data-lang-en="Peer-Reviewed Publications" data-lang-zh="同行评审论文">Peer-Reviewed Publications</span>
                </h2>
                <ul class="publication-list">
                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="ManiNet: Manifold Network for Few-Shot Learning"
                             data-lang-zh="ManiNet：面向小样本学习的流形网络"
                             data-abstract-en="Few-shot Learning (FSL) aims to learn a model that can be seamlessly adapted to unknown classes with only a few labeled data. We develop a novel yet concise approach named Manifold Network (ManiNet) to perform few-shot classification based on manifolds. Technically, in the ManiNet, each class is represented as a tree rather than isolated centroids to reserve structural information."
                             data-abstract-zh="小样本学习（FSL）旨在仅利用少量标注数据即可无缝适应未知类别的模型。我们开发了一种新颖而简洁的方法——流形网络（ManiNet），基于流形进行小样本分类。在技术上，ManiNet将每个类别表示为树结构而非孤立的质心，以保留结构信息。">
                            ManiNet: Manifold Network for Few-Shot Learning
                        </div>
                        <div class="publication-authors">Ruiqi Wang, Hengcan Shi, <span class="highlight">Yi Chen</span>, YaoNan Wang</div>
                        <div class="publication-venue">AIHCIR 2025 (Best Paper Award)</div>
                        <div class="publication-links">
                            <a href="#"><i class="fas fa-file-pdf"></i> PDF</a>
                        </div>
                    </li>

                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly Grounding and Understanding"
                             data-lang-zh="VAGU & GtS：基于大语言模型的视频异常定位与理解联合基准与框架"
                             data-abstract-en="Video Anomaly Detection (VAD) aims to identify anomalous events within videos. We propose VAGU (Video Anomaly Grounding and Understanding), the first benchmark integrating both anomaly grounding and anomaly understanding. Building upon this benchmark, we present Glance then Scrutinize (GtS) - a training-free framework guided by static textual and dynamic textual prompts."
                             data-abstract-zh="视频异常检测（VAD）旨在识别视频中的异常事件。我们提出了VAGU（视频异常定位与理解），这是首个整合了异常定位和异常理解的基准。在此基础上，我们提出了Glance then Scrutinize (GtS) —— 一个由静态和动态文本提示引导的免训练框架。">
                            VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly Grounding and Understanding
                        </div>
                        <div class="publication-authors">Shibo Gao, Peipei Yang, <span class="highlight">Yi Chen</span>, Han Zhu, Yangyang Liu, Wenxin Zhang, Linlin Huang</div>
                        <div class="publication-venue">AAAI 2026</div>
                        <div class="publication-links">
                            <a href="https://arxiv.org/abs/2507.21507"><i class="fas fa-file-pdf"></i> arXiv</a>
                        </div>
                    </li>

                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information"
                             data-lang-zh="可恢复压缩：文本信息引导的多模态视觉Token恢复机制"
                             data-abstract-en="The advancement of LLM-based agents heralds a new perspective for AI for Science (AI4S). We propose a recoverable compression mechanism for multimodal vision tokens, significantly reducing computational cost while maintaining performance."
                             data-abstract-zh="我们提出了一种文本信息引导的多模态视觉Token可恢复压缩机制。该机制通过智能压缩与恢复策略，在保持模型性能的同时，显著降低了计算成本。">
                            Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information
                        </div>
                        <div class="publication-authors"><span class="highlight">Yi Chen</span>, Jian Xu, Xu-Yao Zhang, Wen-Zhuo Liu, Yang-Yang Liu, Cheng-Lin Liu</div>
                        <div class="publication-venue">AAAI 2025</div>
                        <div class="publication-links">
                            <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32229"><i class="fas fa-globe"></i> Link</a>
                            <a href="https://arxiv.org/abs/2409.01179"><i class="fas fa-file-pdf"></i> arXiv</a>
                            <a href="https://github.com/banjiuyufen/Recoverable-Compression"><i class="fab fa-github"></i> Code</a>
                        </div>
                    </li>

                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="Decoupling Layout from Glyph in Online Chinese Handwriting Generation"
                             data-lang-zh="联机中文手写生成中的字形与布局解耦"
                             data-abstract-en="Teaching machines to generate online handwritten text in various styles presents an interesting challenge. We identify that text lines can naturally be divided into two components: layout and glyphs. Based on this division, we designed a text line layout generator coupled with a diffusion-based stylized font synthesizer to address this challenge hierarchically."
                             data-abstract-zh="教机器生成各种风格的联机手写文本是一个有趣的挑战。我们将文本行自然地划分为布局和字形两个部分。基于此划分，我们设计了一个文本行布局生成器，并结合基于扩散模型的风格化字体合成器，以分层方式解决这一挑战。">
                            Decoupling Layout from Glyph in Online Chinese Handwriting Generation
                        </div>
                        <div class="publication-authors">Min-Si Ren, Yan-Ming Zhang, <span class="highlight">Yi Chen</span></div>
                        <div class="publication-venue">ICLR 2025</div>
                        <div class="publication-links">
                            <a href="https://arxiv.org/abs/2410.02309"><i class="fas fa-file-pdf"></i> arXiv</a>
                            <a href="https://github.com/singularityrms/OLHWG"><i class="fab fa-github"></i> Code</a>
                        </div>
                    </li>

                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="Recognition of Online Handwritten Chinese Texts in Any Writing Direction via Stroke Classification Based Over-Segmentation"
                             data-lang-zh="基于笔画分类过切分的任意书写方向联机中文手写文本识别"
                             data-abstract-en="Current mainstream methods are difficult to handle texts in any writing direction. This paper proposes a recognition framework based on over-segmentation which is applicable to text recognition of any writing direction. An improved over-segmentation algorithm is designed based on stroke classification using bidirectional long short-term memory networks (BiLSTM)."
                             data-abstract-zh="目前的主流方法难以处理任意书写方向的文本。本文提出了一种基于过切分的识别框架，适用于任意书写方向的文本识别。我们设计了一种改进的过切分算法，利用双向长短时记忆网络（BiLSTM）进行笔画分类。">
                            Recognition of Online Handwritten Chinese Texts in Any Writing Direction via Stroke Classification Based Over-Segmentation
                        </div>
                        <div class="publication-authors"><span class="highlight">Yi Chen</span>, Heng Zhang, Min-Si Ren, Cheng-Lin Liu</div>
                        <div class="publication-venue">ICPR 2024</div>
                        <div class="publication-links">
                            <a href="https://link.springer.com/chapter/10.1007/978-3-031-78183-4_24"><i class="fas fa-globe"></i> Link</a>
                        </div>
                    </li>

                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="Context-Aware Confidence Estimation for Rejection in Handwritten Chinese Text Recognition"
                             data-lang-zh="手写中文文本识别中用于拒识的上下文感知置信度估计"
                             data-abstract-en="We propose a character confidence estimation method incorporating contexts for character rejection in HCTR. Based on a text line recognizer outputting character segmentation and classification results, the confidence of each segmented character is estimated by combining the scores of a re-trained character classifier, the linguistic and geometric contexts."
                             data-abstract-zh="我们提出了一种结合上下文的字符置信度估计方法，用于手写中文文本识别（HCTR）中的拒识。该方法基于输出字符切分和分类结果的文本行识别器，通过结合重训练的字符分类器分数、语言上下文和几何上下文，来估计每个切分字符的置信度。">
                            Context-Aware Confidence Estimation for Rejection in Handwritten Chinese Text Recognition
                        </div>
                        <div class="publication-authors">Yang-Yang Liu, <span class="highlight">Yi Chen</span>, Fei Yin, Cheng-Lin Liu</div>
                        <div class="publication-venue">ICDAR 2024</div>
                        <div class="publication-links">
                            <a href="https://link.springer.com/chapter/10.1007/978-3-031-70533-5_9"><i class="fas fa-globe"></i> Link</a>
                        </div>
                    </li>

                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="Improved Learning for Online Handwritten Chinese Text Recognition with Convolutional Prototype Network"
                             data-lang-zh="基于卷积原型网络的联机中文手写文本识别改进学习方法"
                             data-abstract-en="We proposed a learning method for segmentation-based online handwritten Chinese text recognition with a convolutional prototype network as the underlying classifier. The prototype classifier is inherently resistant to non-characters, and so, can be trained with character and string samples without the need of data augmentation."
                             data-abstract-zh="我们提出了一种基于切分的联机中文手写文本识别学习方法，采用卷积原型网络作为底层分类器。原型分类器天生具有抗非字符干扰的能力，因此无需数据增强即可利用字符和字符串样本进行训练。">
                            Improved Learning for Online Handwritten Chinese Text Recognition with Convolutional Prototype Network
                        </div>
                        <div class="publication-authors"><span class="highlight">Yi Chen</span>, Heng Zhang, Cheng-Lin Liu</div>
                        <div class="publication-venue">ICDAR 2023</div>
                        <div class="publication-links">
                            <a href="https://link.springer.com/chapter/10.1007/978-3-031-41685-9_3"><i class="fas fa-globe"></i> Link</a>
                        </div>
                    </li>
                </ul>
            </div>
        </div>
        
        <div class="footer">
            <p>© 2025 Yi Chen</p>
            <p>State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS)</p>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // --- Global State ---
            // Try to load saved language, default to 'en'
            let currentLang = localStorage.getItem('lang') || 'en';
            
            // --- Tooltip Logic ---
            const tooltip = document.getElementById('abstract-tooltip');
            const tooltipContent = document.getElementById('tooltip-content');
            let hoverTimer;

            // Function to update tooltip text based on current language
            function updateTooltipText(element) {
                const abstractEn = element.getAttribute('data-abstract-en');
                const abstractZh = element.getAttribute('data-abstract-zh');
                
                if (currentLang === 'zh' && abstractZh) {
                    tooltipContent.textContent = abstractZh;
                } else {
                    tooltipContent.textContent = abstractEn || "Abstract not available.";
                }
            }

            // Function to position tooltip
            function positionTooltip(e) {
                const tooltipWidth = 600; // Match CSS width
                const tooltipMargin = 15;
                
                let left = e.clientX + tooltipMargin;
                let top = e.clientY + tooltipMargin;
                
                // Get viewport dimensions
                const viewportWidth = window.innerWidth;
                const viewportHeight = window.innerHeight;
                
                // Check right boundary
                if (left + tooltipWidth > viewportWidth) {
                    left = e.clientX - tooltipWidth - tooltipMargin;
                }
                
                // Check bottom boundary (approximate tooltip height)
                // Use a safe estimate or calculate dynamically if already displayed
                const estimatedHeight = 200; 
                if (top + estimatedHeight > viewportHeight) {
                    top = e.clientY - estimatedHeight;
                }
                
                // Safety clamp
                if (left < 10) left = 10;
                if (top < 10) top = 10;

                tooltip.style.left = `${left}px`;
                tooltip.style.top = `${top}px`;
            }

            document.querySelectorAll('.publication-title').forEach(title => {
                title.addEventListener('mouseenter', function(e) {
                    // Update text immediately
                    updateTooltipText(this);
                    
                    // Position immediately using clientX/Y (Viewport coordinates)
                    positionTooltip(e);
                    
                    // Show
                    clearTimeout(hoverTimer);
                    tooltip.classList.add('visible');
                });

                title.addEventListener('mousemove', function(e) {
                    // Follow mouse
                    positionTooltip(e);
                });

                title.addEventListener('mouseleave', function() {
                    // Hide
                    tooltip.classList.remove('visible');
                });
            });

            // --- Theme Toggle ---
            const themeToggle = document.getElementById('themeToggle');
            const themeIcon = themeToggle.querySelector('i');
            
            if (localStorage.getItem('theme') === 'dark') {
                document.documentElement.classList.add('dark-mode');
                themeIcon.className = 'fas fa-sun';
            }

            themeToggle.addEventListener('click', () => {
                document.documentElement.classList.toggle('dark-mode');
                const isDark = document.documentElement.classList.contains('dark-mode');
                themeIcon.className = isDark ? 'fas fa-sun' : 'fas fa-moon';
                localStorage.setItem('theme', isDark ? 'dark' : 'light');
            });

            // --- Language Toggle ---
            const langToggle = document.getElementById('langToggle');
            const langText = langToggle.querySelector('span');

            // Initialize Language on Load
            updateLanguageUI();

            function updateLanguageUI() {
                // Update button text
                langText.textContent = currentLang === 'en' ? 'EN' : '中';
                
                // Update all page text
                document.querySelectorAll('.lang-switch').forEach(el => {
                    const text = el.getAttribute(`data-lang-${currentLang}`);
                    if (text) el.innerHTML = text;
                });
            }

            langToggle.addEventListener('click', () => {
                currentLang = currentLang === 'en' ? 'zh' : 'en';
                localStorage.setItem('lang', currentLang);
                updateLanguageUI();
            });
        });
    </script>
</body>
</html>
