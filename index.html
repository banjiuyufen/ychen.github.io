<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yi Chen - Academic Homepage</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&family=Merriweather:wght@300;400;700;900&display=swap" rel="stylesheet">
    <style>
        :root {
            /* Color Palette - Professional & Academic */
            --primary: #0f172a;       /* Deep Navy */
            --primary-light: #334155; 
            --accent: #0369a1;        /* Ocean Blue */
            --accent-hover: #0284c7;
            
            --text-main: #1e293b;
            --text-muted: #475569;
            --text-light: #94a3b8;
            
            --bg-body: #f0f9ff;       /* Very light blue tint */
            --bg-card: #ffffff;
            --border: #e2e8f0;
            
            --shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05), 0 2px 4px -1px rgba(0, 0, 0, 0.01);
            --shadow-hover: 0 20px 25px -5px rgba(0, 0, 0, 0.05), 0 10px 10px -5px rgba(0, 0, 0, 0.01);
            
            --radius: 12px;
            --transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
        }

        :root.dark-mode {
            --primary: #f8fafc;
            --primary-light: #cbd5e1;
            --accent: #38bdf8;        /* Sky Blue */
            --accent-hover: #7dd3fc;
            
            --text-main: #f1f5f9;
            --text-muted: #94a3b8;
            --text-light: #64748b;
            
            --bg-body: #0b1120;       /* Darker Navy */
            --bg-card: #1e293b;
            --border: #334155;
            
            --shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.3);
            --shadow-hover: 0 20px 25px -5px rgba(0, 0, 0, 0.4);
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }

        body {
            font-family: 'Inter', sans-serif;
            background-color: var(--bg-body);
            color: var(--text-main);
            line-height: 1.6;
            transition: var(--transition);
            /* Dynamic Aurora Background Effect */
            background-image: 
                radial-gradient(at 0% 0%, rgba(56, 189, 248, 0.1) 0px, transparent 50%),
                radial-gradient(at 100% 0%, rgba(14, 165, 233, 0.1) 0px, transparent 50%),
                radial-gradient(at 100% 100%, rgba(56, 189, 248, 0.05) 0px, transparent 50%);
            background-attachment: fixed;
            min-height: 100vh;
        }

        h1, h2, h3, .pub-title { font-family: 'Merriweather', serif; }
        a { color: var(--accent); text-decoration: none; transition: var(--transition); }
        a:hover { color: var(--accent-hover); }

        .container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 80px 20px;
        }

        /* Control Bar */
        .control-bar {
            position: fixed;
            top: 24px;
            right: 24px;
            display: flex;
            gap: 12px;
            z-index: 100;
            background: rgba(255, 255, 255, 0.8);
            padding: 8px;
            border-radius: 50px;
            border: 1px solid var(--border);
            box-shadow: var(--shadow);
            backdrop-filter: blur(12px);
        }
        
        .dark-mode .control-bar { background: rgba(30, 41, 59, 0.8); }

        .control-btn {
            background: transparent;
            border: none;
            color: var(--text-muted);
            width: 36px;
            height: 36px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            transition: var(--transition);
            font-weight: 600;
        }

        .control-btn:hover { color: var(--accent); background: rgba(56, 189, 248, 0.1); }

        /* Profile Section */
        .profile-section {
            display: flex;
            gap: 50px;
            margin-bottom: 80px;
            align-items: center;
            background: var(--bg-card);
            padding: 50px;
            border-radius: var(--radius);
            border: 1px solid var(--border);
            box-shadow: var(--shadow);
            position: relative;
            overflow: hidden;
        }
        
        .profile-section::before {
            content: "";
            position: absolute;
            top: 0; left: 0; width: 6px; height: 100%;
            background: linear-gradient(180deg, var(--accent), #7dd3fc);
        }

        .profile-img {
            width: 200px;
            height: 200px;
            object-fit: cover;
            border-radius: var(--radius);
            box-shadow: var(--shadow);
            flex-shrink: 0;
            filter: grayscale(10%);
            transition: var(--transition);
        }
        
        .profile-img:hover { filter: grayscale(0%); transform: scale(1.02); }

        .header-name { font-size: 2.5rem; font-weight: 900; color: var(--primary); margin-bottom: 8px; }
        .header-title { font-size: 1.1rem; color: var(--text-muted); margin-bottom: 24px; font-weight: 400; border-bottom: 1px dashed var(--border); padding-bottom: 24px;}
        
        .social-links { display: flex; gap: 15px; flex-wrap: wrap; margin-top: 25px; }
        .social-btn {
            font-size: 0.9rem; padding: 8px 16px; border-radius: 50px;
            background: var(--bg-body); color: var(--text-muted); border: 1px solid var(--border);
            display: flex; align-items: center; gap: 8px; font-weight: 500;
        }
        .social-btn:hover { border-color: var(--accent); color: var(--accent); transform: translateY(-2px); box-shadow: var(--shadow); }

        /* Section Styling */
        .section-header {
            display: flex;
            align-items: center;
            gap: 15px;
            margin: 60px 0 30px 0;
            padding-bottom: 15px;
            border-bottom: 2px solid var(--border);
        }
        
        .section-header i { color: var(--accent); font-size: 1.4rem; }
        .section-header h2 { font-size: 1.8rem; color: var(--primary); margin: 0; }

        /* Publication List */
        .pub-list { display: flex; flex-direction: column; gap: 24px; }
        
        .pub-item {
            background: var(--bg-card);
            padding: 28px;
            border-radius: var(--radius);
            border: 1px solid var(--border);
            transition: var(--transition);
            position: relative;
        }

        .pub-item:hover {
            transform: translateY(-4px);
            box-shadow: var(--shadow-hover);
            border-color: var(--accent);
        }

        .pub-title {
            font-size: 1.25rem;
            font-weight: 700;
            color: var(--primary);
            cursor: pointer;
            line-height: 1.4;
            display: inline;
            background-image: linear-gradient(var(--accent), var(--accent));
            background-position: 0% 100%;
            background-repeat: no-repeat;
            background-size: 0% 2px;
            transition: background-size .3s;
        }
        
        .pub-title:hover { background-size: 100% 2px; color: var(--accent); }

        .pub-authors { font-size: 1rem; color: var(--text-muted); margin-top: 10px; }
        .pub-authors .me { color: var(--primary); font-weight: 700; }

        .pub-venue {
            font-size: 0.95rem;
            color: var(--text-light);
            font-style: italic;
            margin-top: 8px;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        .highlight-tag {
            background: rgba(251, 191, 36, 0.15);
            color: #d97706;
            padding: 2px 10px;
            border-radius: 4px;
            font-size: 0.8rem;
            font-weight: 700;
            font-style: normal;
            border: 1px solid rgba(251, 191, 36, 0.3);
        }
        
        .dark-mode .highlight-tag { color: #fbbf24; }

        .pub-links { margin-top: 15px; display: flex; gap: 12px; }
        .pub-link {
            font-size: 0.85rem; font-weight: 600; text-transform: uppercase; letter-spacing: 0.5px;
            padding: 6px 14px; border: 1px solid var(--border); border-radius: 6px;
            color: var(--text-muted); background: transparent;
        }
        .pub-link:hover { background: var(--primary); color: var(--bg-card); border-color: var(--primary); }

        /* Tooltip - Supports Full Abstract */
        .tooltip-container {
            position: fixed;
            z-index: 1000;
            background: var(--bg-card);
            width: 650px; /* Wider for reading comfort */
            max-width: 90vw;
            max-height: 80vh; /* Allow tall content */
            overflow-y: auto; /* Scrollable */
            padding: 30px;
            border-radius: var(--radius);
            box-shadow: 0 25px 50px -12px rgba(0, 0, 0, 0.25);
            border: 1px solid var(--border);
            opacity: 0;
            visibility: hidden;
            transition: all 0.25s ease;
            pointer-events: none;
        }
        
        /* Custom Scrollbar for Tooltip */
        .tooltip-container::-webkit-scrollbar { width: 8px; }
        .tooltip-container::-webkit-scrollbar-track { background: var(--bg-body); border-radius: 4px; }
        .tooltip-container::-webkit-scrollbar-thumb { background: var(--text-light); border-radius: 4px; }
        
        .tooltip-container.active { opacity: 1; visibility: visible; pointer-events: auto; }
        
        .tooltip-header {
            font-size: 0.85rem; text-transform: uppercase; letter-spacing: 1.5px;
            color: var(--accent); margin-bottom: 15px; font-weight: 800;
            position: sticky; top: 0; background: var(--bg-card); padding-bottom: 10px;
            border-bottom: 1px solid var(--border);
        }
        
        .tooltip-text { 
            font-size: 1rem; 
            color: var(--text-main); 
            text-align: justify; 
            line-height: 1.8;
            font-family: 'Inter', sans-serif;
        }

        /* Footer */
        footer {
            text-align: center; margin-top: 100px; padding: 40px 0;
            border-top: 1px solid var(--border); color: var(--text-light); font-size: 0.95rem;
        }

        @media (max-width: 768px) {
            .profile-section { flex-direction: column; text-align: center; padding: 30px 20px; }
            .header-name { font-size: 2rem; }
            .social-links { justify-content: center; }
            .container { padding: 40px 15px; }
            .tooltip-container { width: 95vw; bottom: 0; top: auto; transform: none !important; border-radius: 12px 12px 0 0; max-height: 70vh; }
        }
    </style>
</head>
<body>

    <div class="control-bar">
        <button class="control-btn" id="themeToggle" title="Toggle Theme"><i class="fas fa-moon"></i></button>
        <button class="control-btn" id="langToggle" title="Switch Language"><span style="font-size:0.8rem; font-weight:800;">CN</span></button>
    </div>

    <div id="abstract-tooltip" class="tooltip-container">
        <div class="tooltip-header">Abstract</div>
        <div class="tooltip-text" id="tooltip-content"></div>
    </div>

    <div class="container">
        
        <header class="profile-section">
            <img src="ychen.jpg" alt="Yi Chen" class="profile-img">
            <div style="flex: 1;">
                <h1 class="header-name">Yi Chen</h1>
                <div class="header-title lang-text" 
                     data-en="PhD Candidate in Pattern Recognition and Intelligent Systems"
                     data-zh="模式识别与智能系统 博士候选人">
                     PhD Candidate in Pattern Recognition and Intelligent Systems
                </div>
                <div class="lang-text" 
                     data-en="I am currently a PhD student jointly affiliated with the <strong>State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS)</strong> at the Institute of Automation, Chinese Academy of Sciences (CASIA) and Zhongguancun Academy. I am supervised by Prof. <a href='#'>Cheng-Lin Liu</a>. My research focuses on <strong>AI for Science</strong> (specifically Adjuvant Discovery) and the fundamental theories of <strong>Multimodal Large Language Models</strong>."
                     data-zh="我目前就读于中国科学院自动化研究所<strong>多模态人工智能系统国家重点实验室 (MAIS)</strong> 和中关村学院。师从<a href='#'>刘成林</a>教授。我的研究主要集中在 <strong>AI for Science</strong>（特别是佐剂发现）以及<strong>多模态大模型</strong>的基础理论。">
                     Jointly affiliated with <strong>MAIS, CASIA</strong> and Zhongguancun Academy. Supervised by Prof. <a href="#">Cheng-Lin Liu</a>.
                </div>
                
                <div class="social-links">
                    <a href="https://scholar.google.com/citations?user=XoiT9wMAAAAJ&hl=zh-CN" class="social-btn"><i class="fab fa-google"></i> Google Scholar</a>
                    <a href="https://github.com/banjiuyufen" class="social-btn"><i class="fab fa-github"></i> GitHub</a>
                    <a href="mailto:yi.chen@nlpr.ia.ac.cn" class="social-btn"><i class="fas fa-envelope"></i> Email</a>
                </div>
            </div>
        </header>

        <div class="section-header">
            <i class="fas fa-file-invoice"></i>
            <h2 class="lang-text" data-en="Preprints & Under Review" data-zh="预印本与在投论文">Preprints & Under Review</h2>
        </div>
        
        <div class="pub-list">
            <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="An Open-Ended Benchmark and Formal Framework for Adjuvant Research with MLLM"
                     data-title-zh="基于多模态大模型的佐剂研究开放式基准与形式化框架"
                     data-abs-en="Adjuvants play a critical role in modulating immune responses and are central to the development of vaccines and immunotherapies. Yet progress in this field is constrained by data scarcity and incomplete understanding of mechanisms of action, which limit the transition from experience-based design to AI-driven approaches. To address these challenges, we present the first benchmark dedicated to adjuvants, constructed in an open-ended Q&A format and annotated by domain experts. The benchmark comprises 1,294 Q&A pairs and 1,364 formal descriptions, providing a resource for evaluating general-purpose multimodal large language models (MLLMs) and for developing domain-specific systems. We systematically assess 11 closed-source and 18 open-source MLLMs across dimensions including domain-specific Q&A, hallucination rejection, data generation, and instruction following. Results indicate that OpenAI-o1 (STS = 0.7495, LLM Score = 7.7) and DeepSeek-R1 (STS = 0.7415, LLM Score = 7.7) achieved the strongest performance among closed- and open-source models, respectively. In addition, we introduce a formal description framework for representing adjuvant design principles and immune mechanisms as structured abstractions, which can serve as building blocks for future domain-specialized MLLMs. Overall, this work provides a first step toward systematically integrating MLLMs into adjuvant research by offering a dedicated benchmark, comparative evaluation of existing models, and a formal foundation for future development."
                     data-abs-zh="佐剂在调节免疫反应中起着关键作用，是疫苗和免疫疗法开发的核心。然而，该领域的进展受到数据稀缺和作用机制理解不完全的限制，阻碍了从基于经验的设计向AI驱动方法的转变。为解决这些挑战，我们提出了首个专门针对佐剂的基准测试，该基准以开放式问答格式构建，并由领域专家标注。该基准包含1,294个问答对和1,364个形式化描述，为评估通用多模态大语言模型（MLLMs）和开发领域特定系统提供了资源。我们从领域特定问答、幻觉拒绝、数据生成和指令跟随等维度系统评估了11个闭源和18个开源MLLM。结果表明，OpenAI-o1（STS = 0.7495，LLM评分 = 7.7）和DeepSeek-R1（STS = 0.7415，LLM评分 = 7.7）分别在闭源和开源模型中取得了最强性能。此外，我们引入了一个形式化描述框架，将佐剂设计原则和免疫机制表示为结构化抽象，可作为未来领域专用MLLM的构建模块。总体而言，这项工作通过提供专用基准、现有模型的比较评估以及未来发展的形式化基础，迈出了将MLLM系统整合到佐剂研究中的第一步。">
                    An Open-Ended Benchmark and Formal Framework for Adjuvant Research with MLLM
                </div>
                <div class="pub-authors"><span class="me">Yi Chen*</span>, Yu Zhang*, Jian Xu, Xu-Yao Zhang, Hua Yue, Xinming Wang, Zequan Lyu, Wei Wei, Cheng-Lin Liu</div>
                <div class="pub-venue">arXiv Preprint</div>
            </div>

            <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="Sparsity Meets Similarity: Leveraging Long-Tail Distribution for Dynamic Optimized Token Representation in Multimodal Large Language Models"
                     data-title-zh="稀疏性与相似性的融合：利用长尾分布优化多模态大模型Token表示"
                     data-abs-en="Recently, multimodal large language models (MM-LLMs) have achieved significant success in various tasks, but their high computational costs limit widespread application. The main computational burden arises from processing concatenated text and visual tokens in the LLM layer, where input token length directly affects efficiency. Our analysis of visual tokens reveals that their similarity to the CLS token follows a long-tail distribution, with only a few showing high similarity. To address this, we propose a dynamic pruning algorithm that identifies the inflection point in the visual CLS token similarity curve, enabling effective trimming of visual markers to accelerate model performance. Additionally, we perform a second round of pruning in the LLM layer, filtering out low-correlation tokens through the interaction between visual and textual features. Experimental results demonstrate that our method achieves performance comparable to the original while utilizing only 22% of the original token quantity. Our source code will be made publicly available upon acceptance."
                     data-abs-zh="近年来，多模态大语言模型（MM-LLMs）在各项任务中取得了显著成功，但其高昂的计算成本限制了广泛应用。主要的计算负担源于在LLM层处理拼接的文本和视觉Token，其中输入Token的长度直接影响效率。我们对视觉Token的分析表明，它们与CLS Token的相似度遵循长尾分布，只有少数表现出高相似度。为解决这一问题，我们提出了一种动态剪枝算法，该算法识别视觉CLS Token相似度曲线中的拐点，从而能够有效修剪视觉标记以加速模型性能。此外，我们在LLM层进行第二轮剪枝，通过视觉和文本特征之间的交互过滤掉低相关性的Token。实验结果表明，我们的方法在仅使用原始Token数量22%的情况下，实现了与原始模型相当的性能。我们的源代码将在录用后公开。">
                    Sparsity Meets Similarity: Leveraging Long-Tail Distribution for Dynamic Optimized Token Representation
                </div>
                <div class="pub-authors"><span class="me">Yi Chen*</span>, Gao-Tong Yu*, Jian Xu</div>
                <div class="pub-venue">arXiv Preprint</div>
            </div>

            <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="An Efficient Strategy for Data-constrained Machine Learning in Materials Science"
                     data-title-zh="材料科学中数据受限机器学习的高效策略"
                     data-abs-en="Materials science research increasingly benefits from the application of machine learning method, yet encounters fundamental challenges from data scarcity, such as limited dataset sizes and severe data distribution imbalance. In this paper, we develop a multi-task and auxiliary machine learning framework to address these limitations. Using the 2D materials dataset as a case study, our approach demonstrates significantly enhanced prediction accuracy over the baseline crystal graph convolutional neural networks method. Specifically, it reduced the mean absolute error for band gap prediction from 0.30 eV to 0.23 eV and for work function from 0.34 eV to 0.24 eV. The performance gain stems from the framework’s ability to effectively exploit underlying physical correlations between material properties through synergistic multi-task and auxiliary learning. Furthermore, its modular architecture enables seamless integration with various graph based or other end-to-end deep learning models, presenting a computationally efficient and easily implementable solution for constrained datasets in material science."
                     data-abs-zh="材料科学研究日益受益于机器学习方法的应用，但仍面临数据稀缺（如数据集规模有限和数据分布严重不平衡）带来的根本挑战。在本文中，我们开发了一个多任务辅助机器学习框架来解决这些局限性。以二维材料数据集为例，我们的方法显示出的预测精度显著优于基准晶体图卷积神经网络方法。具体而言，它将带隙预测的平均绝对误差从0.30 eV降低到0.23 eV，将功函数预测的误差从0.34 eV降低到0.24 eV。性能的提升源于该框架通过协同多任务和辅助学习有效利用材料属性之间潜在物理相关性的能力。此外，其模块化架构能够与各种基于图或其他端到端深度学习模型无缝集成，为材料科学中的受限数据集提供了一种计算高效且易于实施的解决方案。">
                     An Efficient Strategy for Data-constrained Machine Learning in Materials Science
                </div>
                <div class="pub-authors">ChunTing Shao*, <span class="me">Yi Chen*</span>, ShanMan Song, PeiPei Yang, QingBo Yan, Jian Xu, Gang Su</div>
                <div class="pub-venue">Under Review</div>
            </div>

            <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="ContextRGBNav: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation"
                     data-title-zh="ContextRGBNav：基于上下文适应的单目零样本语义导航框架"
                     data-abs-en="Efficiently finding objects in complex environments is fundamental to real-world embodied applications. While recent advances in multimodal foundation models have enabled zero-shot object goal navigation, allowing robots to search for arbitrary objects without fine-tuning, existing methods face two key limitations: (1) heavy reliance on precise depth and pose information provided by simulators, which restricts applicability in real-world scenarios; and (2) lack of in-context learning (ICL) capability, making it difficult to quickly adapt to new environments from a short video. To address these challenges, we propose ContextRGBNav, a novel zero-shot, open-vocabulary semantic navigation framework that operates using only a monocular camera. Leveraging powerful 3D foundation models, ContextRGBNav eliminates the dependency on depth and pose while exhibiting strong ICL capability: by simply observing a short video of a new environment, the system significantly improves task efficiency without requiring architectural modifications or fine-tuning. The framework integrates several key components: keyframe-based 3D reconstruction, semantic point cloud generation, vision-language model (VLM)-driven exploration value estimation, high-level adaptive waypoint selection, and low-level action execution. Experiments on the HM3D benchmark and real-world environments demonstrate that ContextRGBNav achieves competitive performance in terms of navigation success rate and exploration efficiency, while showing superior ICL adaptability."
                     data-abs-zh="在复杂环境中高效寻找物体是现实世界具身应用的基础。虽然多模态基础模型的最新进展实现了零样本物体目标导航，允许机器人在无需微调的情况下搜索任意物体，但现有方法面临两个主要限制：（1）严重依赖模拟器提供的精确深度和位姿信息，限制了在现实场景中的适用性；（2）缺乏上下文学习（ICL）能力，难以通过短视频快速适应新环境。为解决这些挑战，我们提出了ContextRGBNav，这是一种新颖的、仅使用单目摄像头的零样本、开放词汇语义导航框架。利用强大的3D基础模型，ContextRGBNav消除了对深度和位姿的依赖，同时展现出强大的ICL能力：只需观察新环境的短视频，系统即可在无需架构修改或微调的情况下显著提高任务效率。该框架集成了几个关键组件：基于关键帧的3D重建、语义点云生成、视觉语言模型（VLM）驱动的探索价值估计、高级自适应航点选择和低级动作执行。在HM3D基准和现实环境中的实验表明，ContextRGBNav在导航成功率和探索效率方面取得了具有竞争力的性能，同时展现出卓越的ICL适应性。">
                     ContextRGBNav: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation
                </div>
                <div class="pub-authors">Ming-Ming Yu, <span class="me">Yi Chen</span>, Börje F. Karlsson, Wenjun Wu</div>
                <div class="pub-venue">arXiv Preprint</div>
            </div>

            <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning"
                     data-title-zh="ChartAgent：基于工具集成推理的图表理解框架"
                     data-abs-en="With high information density and intuitive readability, charts have become the de facto standard medium for data analysis and communication across disciplines. Although recent multimodal large language models (MLLMs) have achieved notable advances in automated chart understanding, they still exhibit a pronounced dependence on explicit textual annotations; when key numerals are absent, performance degrades markedly. To overcome this limitation, we propose ChartAgent, a chart understanding framework built upon Tool Integrated Reasoning (TIR). Inspired by human cognitive processes, ChartAgent decomposes complex chart-analysis tasks into a sequence of observable and replayable steps. To support this architecture, we develop an extensible, modular visual tool library that encapsulates more than a dozen core tools—including key element detection, instance segmentation, and optical character recognition (OCR)—which the agent dynamically orchestrates to achieve systematic visual parsing across diverse chart types. Leveraging the transparency and verifiability of TIR, ChartAgent moves the decision process beyond the black box paradigm: intermediate outputs from individual tools are standardized and consolidated into an Evidence Package, providing traceable and reproducible support for final conclusions. Experimental results demonstrate that ChartAgent significantly improves chart understanding robustness under sparse-annotation settings, offering a practical pathway toward trustworthy and reliable systems for data-visualization analytics."
                     data-abs-zh="凭借高信息密度和直观的可读性，图表已成为跨学科数据分析和交流的事实标准媒介。尽管最近的多模态大语言模型（MLLMs）在自动图表理解方面取得了显著进展，但它们仍表现出对显式文本注释的明显依赖；当关键数字缺失时，性能会显著下降。为克服这一限制，我们提出了ChartAgent，这是一个建立在工具集成推理（TIR）之上的图表理解框架。受人类认知过程的启发，ChartAgent将复杂的图表分析任务分解为一系列可观察和可重放的步骤。为支持该架构，我们开发了一个可扩展的模块化视觉工具库，封装了十多种核心工具——包括关键元素检测、实例分割和光学字符识别（OCR）——智能体可动态编排这些工具，以实现对各种图表类型的系统化视觉解析。利用TIR的透明性和可验证性，ChartAgent将决策过程移出了黑箱范式：单个工具的中间输出被标准化并整合到证据包中，为最终结论提供可追溯和可复现的支持。实验结果表明，ChartAgent显著提高了稀疏注释设置下图表理解的鲁棒性，为可信可靠的数据可视化分析系统提供了一条切实可行的途径。">
                     ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning
                </div>
                <div class="pub-authors">Boran Wang, Xinming Wang, <span class="me">Yi Chen</span>, Xiang Li, Jian Xu, Jing Yuan, Cheng-Lin Liu</div>
                <div class="pub-venue">arXiv Preprint</div>
            </div>

            <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="MR-ALIGN: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models"
                     data-title-zh="MR-ALIGN：面向大型推理模型的元推理引导事实性对齐"
                     data-abs-en="Large reasoning models (LRMs) show strong capabilities in complex reasoning, yet their marginal gains on evidence-dependent factual questions are limited. We find this limitation is partially attributable to a reasoning–answer hit gap, where the model identifies the correct facts during reasoning but fails to incorporate them into the final response, thereby reducing factual fidelity. To address this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment framework that enhances factuality without relying on external verifiers. MR-ALIGN quantifies state-transition probabilities along the model’s thinking process and constructs a transition-aware implicit reward that reinforces beneficial reasoning patterns while suppressing defective ones at the atomic thinking segments. This re-weighting reshapes token-level signals into probability-aware segment scores, encouraging coherent reasoning trajectories that are more conducive to factual correctness. Empirical evaluations across four factual QA datasets and one long-form factuality benchmark show that MR-ALIGN consistently improves accuracy and truthfulness while reducing misleading reasoning. These results highlight that aligning the reasoning process itself, rather than merely the outputs, is pivotal for advancing factuality in LRMs."
                     data-abs-zh="大型推理模型（LRMs）在复杂推理中显示出强大的能力，但在依赖证据的事实性问题上的边际收益有限。我们发现这种限制部分归因于推理-答案命中差距，即模型在推理过程中识别出了正确事实，但未能将其整合到最终回答中，从而降低了事实保真度。为解决这一问题，我们提出了MR-ALIGN，这是一个元推理引导的对齐框架，无需依赖外部验证器即可增强事实性。MR-ALIGN量化模型思维过程中的状态转移概率，并构建转移感知隐式奖励，在原子思维片段层面增强有益的推理模式并抑制有缺陷的模式。这种重加权将Token级信号重塑为概率感知片段分数，鼓励更有利于事实正确性的连贯推理轨迹。在四个事实问答数据集和一个长篇事实性基准上的实证评估表明，MR-ALIGN一致地提高了准确性和真实性，同时减少了误导性推理。这些结果强调，对齐推理过程本身而非仅仅对齐输出，对于推进LRM的事实性至关重要。">
                     MR-ALIGN: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models
                </div>
                <div class="pub-authors">Xinming Wang, Jian Xu, Bin Yu, Sheng Lian, Hongzhu Yi, <span class="me">Yi Chen</span>, et al.</div>
                <div class="pub-venue">arXiv Preprint</div>
            </div>

            <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="One Patch Doesn’t Fit All: Adaptive Patching for Native-Resolution Multimodal Large Language Models"
                     data-title-zh="One Patch Doesn’t Fit All：原生分辨率多模态大模型的自适应分块"
                     data-abs-en="Real-world visual signals are inherently variable in resolution, and it is natural to endow multimodal large language models (MLLMs) with such native-resolution perception capabilities. In principle, for general and straightforward multimodal understanding, low-resolution images are sufficient. While for images with nuanced details like documents and charts, it is crucial to preserve fine-grained details using high-resolution inputs, as naive resizing inevitably results in information loss. Recent advances employ sequence packing to process images of any resolution and aspect ratios. Despite these efforts, model performance degrades at both low and high resolutions, and high-resolution inputs incur substantial computational costs. We argue that the rigid use of a single patch size is the primary cause: when image resolution or information density varies, fixing patch size is intrinsically suboptimal. To address this issue, we introduce Adaptive Patching (AdaPatch), a simple yet effective strategy that adjusts patch size according to image resolution and information density and could be seamlessly plugged into pre-trained fixed-patch MLLMs without any training efforts. Extensive evaluations demonstrate consistent improvements in native resolution performance without additional training. Besides, we provide a training-based method to further adapt MLLMs with dynamic patch sizes and enhance the performance."
                     data-abs-zh="现实世界的视觉信号在分辨率上本质上是可变的，赋予多模态大语言模型（MLLMs）这种原生分辨率感知能力是很自然的。原则上，对于一般和直接的多模态理解，低分辨率图像就足够了。然而，对于具有细微细节的图像（如文档和图表），使用高分辨率输入来保留细粒度细节至关重要，因为简单的缩放不可避免地会导致信息丢失。最近的进展采用了序列打包来处理任何分辨率和纵横比的图像。尽管做出了这些努力，模型性能在低分辨率和高分辨率下都会下降，且高分辨率输入会产生巨大的计算成本。我们要指出，僵化地使用单一分块大小是主要原因：当图像分辨率或信息密度变化时，固定分块大小本质上是次优的。为解决这个问题，我们引入了自适应分块（AdaPatch），这是一种简单而有效的策略，它根据图像分辨率和信息密度调整分块大小，并且可以无缝插入预训练的固定分块MLLM中，无需任何训练工作。广泛的评估表明，无需额外训练即可在原生分辨率性能上取得一致的改进。此外，我们提供了一种基于训练的方法，以进一步使MLLM适应动态分块大小并提高性能。">
                     One Patch Doesn’t Fit All: Adaptive Patching for Native-Resolution Multimodal Large Language Models
                </div>
                <div class="pub-authors">Wenzhuo Liu, Weijie Yin, Fei Zhu, Shijie Ma, Haiyang Guo, <span class="me">Yi Chen</span>, et al.</div>
                <div class="pub-venue">arXiv Preprint</div>
            </div>

            <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="The Evolution of Video Anomaly Detection: A Unified Framework from DNN to MLLM"
                     data-title-zh="视频异常检测的演变：从DNN到MLLM的统一框架"
                     data-abs-en="Video anomaly detection (VAD) aims to identify and ground anomalous behaviors or events in videos, serving as a core technology in the fields of intelligent surveillance and public safety. With the advancement of deep learning, the continuous evolution of deep model architectures has driven innovation in VAD methodologies, significantly enhancing feature representation and scene adaptability, thereby improving algorithm generalization and expanding application boundaries. More importantly, the rapid development of multi-modal large language (MLLMs) and large language models (LLMs) has introduced new opportunities and challenges to the VAD field. Under the support of MLLMs and LLMs, VAD has undergone significant transformations in terms of data annotation, input modalities, model architectures, and task objectives. The surge in publications and the evolution of tasks have created an urgent need for systematic reviews of recent advancements. This paper presents the first comprehensive survey analyzing VAD methods based on MLLMs and LLMs, providing an in-depth discussion of the changes occurring in the VAD field in the era of large models and their underlying causes. Additionally, this paper proposes a unified framework that encompasses both deep neural network (DNN)-based and LLM-based VAD methods, offering a thorough analysis of the new VAD paradigms empowered by LLMs, constructing a classification system, and comparing their strengths and weaknesses. Building on this foundation, this paper focuses on current VAD methods based on MLLMs/LLMs. Finally, based on the trajectory of technological advancements and existing bottlenecks, this paper distills key challenges and outlines future research directions, offering guidance for the VAD community."
                     data-abs-zh="视频异常检测（VAD）旨在识别和定位视频中的异常行为或事件，是智能监控和公共安全领域的核心技术。随着深度学习的进步，深度模型架构的不断演进推动了VAD方法的创新，显著增强了特征表示和场景适应性，从而提高了算法泛化能力并扩展了应用边界。更重要的是，多模态大语言模型（MLLMs）和大语言模型（LLMs）的快速发展为VAD领域带来了新的机遇和挑战。在MLLMs和LLMs的支持下，VAD在数据标注、输入模态、模型架构和任务目标方面经历了重大变革。出版物的激增和任务的演变使得对近期进展进行系统综述变得迫切。本文首次全面综述了基于MLLM和LLM的VAD方法，深入探讨了大模型时代VAD领域发生的变化及其根本原因。此外，本文提出了一个包含基于深度神经网络（DNN）和基于LLM的VAD方法的统一框架，彻底分析了LLM赋能的新VAD范式，构建了分类系统，并比较了它们的优缺点。在此基础上，本文重点关注当前基于MLLM/LLM的VAD方法。最后，基于技术进步的轨迹和现有瓶颈，本文提炼了关键挑战并概述了未来的研究方向，为VAD社区提供指导。">
                     The Evolution of Video Anomaly Detection: A Unified Framework from DNN to MLLM
                </div>
                <div class="pub-authors">Shibo Gao, Peipei Yang, Haiyang Guo, Yangyang Liu, <span class="me">Yi Chen</span>, et al.</div>
                <div class="pub-venue">arXiv Preprint</div>
            </div>

             <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction"
                     data-title-zh="MeteorPred：用于极端天气事件预测的气象多模态大模型与数据集"
                     data-abs-en="Timely and accurate severe weather alerts are critical for disaster mitigation. However, current forecasting systems rely largely on expert knowledge, resulting in subjectivity and high labor demands. With the rapid development of AI technologies, the end-to-end AI weather station is gradually emerging as a new trend in predicting severe weather events. Three core challenges impede the development of end-to-end AI severe weather system: (1) scarcity of severe weather event samples; (2) imperfect alignment between high-dimensional meteorological data and textual warnings; (3) existing multimodal language models are unable to handle high-dimensional meteorological data and struggle to fully capture the complex dependencies across temporal sequences, vertical pressure levels, and spatial dimensions. To address these challenges, we introduce MP-Bench, the first large-scale temporal multimodal dataset for severe weather events prediction, comprising 421,363 pairs of raw multi-year meteorological data and corresponding text caption, covering a wide range of severe weather scenarios across China. On top of this dataset, we develop a meteorology multimodal large model (MMLM) that directly ingests 4D meteorological inputs. In addition, it is designed to accommodate the unique characteristics of 4D meteorological data flow, incorporating three plug-and-play adaptive fusion modules that enable dynamic feature extraction and integration across modalities, temporal sequences, vertical pressure layers, and spatial dimensions. Extensive experiments on MP-Bench demonstrate that MMLM performs exceptionally well across multiple tasks, highlighting its effectiveness in severe weather understanding and marking a key step toward realizing automated, AI-driven weather forecasting systems."
                     data-abs-zh="及时准确的极端天气预警对减灾至关重要。然而，目前的预报系统很大程度上依赖于专家知识，导致主观性和高人力需求。随着人工智能技术的快速发展，端到端人工智能气象站逐渐成为预测极端天气事件的新趋势。三个核心挑战阻碍了端到端AI极端天气系统的发展：（1）极端天气事件样本稀缺；（2）高维气象数据与文本预警之间的对齐不完善；（3）现有的多模态语言模型无法处理高维气象数据，且难以完全捕捉跨时间序列、垂直压力层和空间维度的复杂依赖关系。为解决这些挑战，我们推出了MP-Bench，这是首个用于极端天气事件预测的大规模时间多模态数据集，包含421,363对原始多年气象数据和相应的文本描述，涵盖了中国各地的广泛极端天气场景。在该数据集的基础上，我们开发了一种气象多模态大模型（MMLM），该模型直接摄取4D气象输入。此外，它专为适应4D气象数据流的独特特性而设计，集成了三个即插即用的自适应融合模块，能够跨模态、时间序列、垂直压力层和空间维度进行动态特征提取和集成。在MP-Bench上的广泛实验表明，MMLM在多项任务中表现出色，突显了其在极端天气理解方面的有效性，并标志着朝着实现自动化、AI驱动的天气预报系统迈出了关键一步。">
                     MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction
                </div>
                <div class="pub-authors">Shuo Tang, Jian Xu, Jiadong Zhang, <span class="me">Yi Chen</span>, et al.</div>
                <div class="pub-venue">arXiv Preprint</div>
            </div>

            <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="The Hitchhiker's Guide to Scientific Agents: A Journey Through the Cosmos of Research Automation"
                     data-title-zh="科学智能体银河漫游指南：研究自动化宇宙之旅"
                     data-abs-en="The advancement of LLM-based agents heralds a new perspective for AI for Science (AI4S), automation science research. Prominent large language models, such as DeepSeek-R1 and OpenAI-o1, have exhibited expertise across multiple domains, prompting researchers to develop agents for the natural sciences and investigate the frontiers of scientific knowledge. Nevertheless, the divergences between the natural sciences and AI have hindered the development and advancement of scientific agents across various fields. This survey is grounded in the standardized scientific research process and elucidates the construction and evaluation of scientific agents. Initially, we delineate the substantial distinctions between scientific agents and general-purpose agents with regard to goal orientation, and presents a systematic taxonomy based on construction strategies and capability scopes. Subsequently, we examine the fundamental procedure for constructing scientific agents from inception and the strategy for targeted capability enhancement within a dual-layer progressive framework. Additionally, we outline the benchmarking and evaluation approaches for scientific agents at different stages of science research. Ultimately, we investigate prospective research directions for scientific agents. It is our aspiration that this survey will assist researchers across diverse research fields in constructing specialized scientific agents, while fostering innovation in the realm of scientific agents to further AI-driven automation science research."
                     data-abs-zh="基于大语言模型的智能体的进步预示着AI for Science (AI4S) 和自动化科学研究的新视角。DeepSeek-R1和OpenAI-o1等杰出的大语言模型已在多个领域展示了专业知识，促使研究人员开发用于自然科学的智能体并探索科学知识的前沿。然而，自然科学与人工智能之间的差异阻碍了各个领域科学智能体的开发和进步。本综述立足于标准化科学研究过程，阐明了科学智能体的构建和评估。首先，我们就目标导向划分了科学智能体与通用智能体之间的实质性区别，并基于构建策略和能力范围提出了系统的分类法。随后，我们考察了从头开始构建科学智能体的基本程序，以及在双层渐进框架内进行针对性能力增强的策略。此外，我们概述了在科学研究不同阶段对科学智能体进行基准测试和评估的方法。最后，我们探讨了科学智能体的未来研究方向。我们期望本综述能帮助不同研究领域的研究人员构建专门的科学智能体，同时促进科学智能体领域的创新，以进一步推动AI驱动的自动化科学研究。">
                     The Hitchhiker's Guide to Scientific Agents: A Journey Through the Cosmos of Research Automation
                </div>
                <div class="pub-authors">Xinming Wang, Aslan Feng, Jian Xu, <span class="me">Yi Chen</span>, et al.</div>
                <div class="pub-venue">TechRxiv</div>
            </div>

            <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="Fine-Grained Post-Training Quantization for Large Vision Language Models with Integrated Gradients"
                     data-title-zh="基于积分梯度的多模态大模型细粒度后训练量化"
                     data-abs-en="Large Vision Language Models (LVLMs) have made breakthrough in various downstream areas that requires multi-modal interaction. However, the powerful capabilities come with a surge in computational overhead and memory usage, which hinders the practical deployment. Among numerous acceleration techniques, quantization is a popular strategy to effectively reduce memory cost and accelerate inference. Despite the great progress in LVLM quantization, exiting methods mainly measure the sensitivity of input tokens based on modality, which fails to express the rich and complicated information between inputs, highlighting the need to quantize from a fine-grained and interpretable way. Drawn from the concept of axiomatic attribution from mechanistic interpretability, we exploit integrated gradients to effectively evaluate the sensitivity, and push the granularity from modality to token to reflect both inter-modality and intra-modality dynamics. We conduct comprehensive experiments under W4A8 and W3A16 quantization for various LVLMs, and the results show that only a 6% increase in quantization time brings substantial performance improvements, demonstrating the superiority of our method."
                     data-abs-zh="大型视觉语言模型（LVLMs）在需要多模态交互的各个下游领域取得了突破。然而，强大的能力伴随着计算开销和内存使用的激增，这阻碍了实际部署。在众多加速技术中，量化是一种有效降低内存成本和加速推理的流行策略。尽管LVLM量化取得了很大进展，但现有方法主要基于模态来衡量输入Token的敏感性，无法表达输入之间丰富而复杂的信息，突显了以细粒度和可解释方式进行量化的必要性。借鉴机理可解释性中的公理归因概念，我们利用积分梯度有效地评估敏感性，并将粒度从模态推向Token，以反映模态间和模态内的动态。我们在W4A8和W3A16量化下对各种LVLM进行了全面实验，结果表明，仅增加6%的量化时间就能带来实质性的性能提升，证明了我们方法的优越性。">
                     Fine-Grained Post-Training Quantization for Large Vision Language Models with Integrated Gradients
                </div>
                <div class="pub-authors">Ziwen Xiang, Fanhu Zeng, Hongjian Fang, Rui-Qi Wang, Renxing Chen, <span class="me">Yi Chen</span>, et al.</div>
                <div class="pub-venue">arXiv Preprint</div>
            </div>
        </div>

        <div class="section-header">
            <i class="fas fa-book-journal-whills"></i>
            <h2 class="lang-text" data-en="Peer-Reviewed Publications" data-zh="正式发表论文">Peer-Reviewed Publications</h2>
        </div>

        <div class="pub-list">
            <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="ManiNet: Manifold Network for Few-Shot Learning"
                     data-title-zh="ManiNet：基于流形的少样本学习网络"
                     data-abs-en="Few-shot Learning (FSL) aims to learn a model that can be seamlessly adapted to unknown classes with only a few labeled data. A concise but successful way is to learn a robust feature encoder to describe novel classes relying on given supervised data. Under the guidance of such insight, most methods define classes as standard Gaussian distributions with different means in feature spaces, where classification can be performed based on the distances between embedding and class centroids. In spite of considerable achievements, these methods always miss the structural information within classes, resulting in degraded performance. To tackle this problem, we develop a novel yet concise approach named Manifold Network (ManiNet) to perform few-shot classification based on manifolds. Technically, in the ManiNet, each class is represented as a tree rather than an isolated centroids to reserve the structural information. And a simple correction term is introduced to elevate the usage of data by representing each manifold with a graph. Benefiting from such modeling, the probability of unknown data belonging to a class is derived based on the relative energy change before and after adding this data into the class manifolds. Experimental results on popular benchmarks strongly demonstrate that our ManiNet suffices to achieve competitive performance with simpler modeling and higher robustness, compared to the previous state-of-the-art."
                     data-abs-zh="少样本学习（FSL）旨在学习一个仅需少量标记数据即可无缝适应未知类别的模型。一种简洁但成功的方法是依靠给定的监督数据学习鲁棒的特征编码器来描述新类别。在这种见解的指导下，大多数方法将类别定义为特征空间中具有不同均值的标准高斯分布，其中分类可以基于嵌入与类别质心之间的距离进行。尽管取得了可观的成就，但这些方法总是忽略类别内的结构信息，导致性能下降。为解决这一问题，我们开发了一种名为流形网络（ManiNet）的新颖而简洁的方法，基于流形进行少样本分类。在技术上，在ManiNet中，每个类被表示为一棵树而不是孤立的质心，以保留结构信息。并且引入了一个简单的修正项，通过用图表示每个流形来提高数据的利用率。得益于这种建模，未知数据属于某个类别的概率是基于将该数据加入类别流形前后的相对能量变化推导出来的。在流行基准上的实验结果有力地证明，与先前的最先进技术相比，我们的ManiNet能够以更简单的建模和更高的鲁棒性实现具有竞争力的性能。">
                    ManiNet: Manifold Network for Few-Shot Learning
                </div>
                <div class="pub-authors">Ruiqi Wang, Hengcan Shi, <span class="me">Yi Chen</span>, YaoNan Wang</div>
                <div class="pub-venue">AIHCIR 2025 <span class="highlight-tag">Best Paper Award</span></div>
            </div>

            <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly Grounding and Understanding"
                     data-title-zh="VAGU & GtS：基于大语言模型的视频异常定位与理解联合基准及框架"
                     data-abs-en="Video Anomaly Detection (VAD) aims to identify anomalous events within videos and precisely ground their temporal occurrences. Current VAD research can be primarily categorized into two paradigms: traditional DNN-based methods predominantly focus on temporal grounding of anomalies, while emerging methods leveraging Large Language Models(LLMs) emphasize the semantic understanding of anomalous content. In fact, both video anomaly understanding and video anomaly grounding tasks are crucial for the comprehensive detection of anomalies in videos. Furthermore, these two tasks can mutually support and corroborate each other. However, there is currently no model capable of simultaneously accomplishing both tasks, nor is there a dataset that can support both tasks. In this paper, we propose VAGU (Video Anomaly Grounding and Understanding), the first benchmark integrating both anomaly grounding and anomaly understanding. Specifically, each instance in VAGU is annotated with three manually curated components: anomaly category specification, anomaly semantic understanding, and precise anomaly temporal grounding. Building upon this benchmark, we present Glance then Scrutinize (GtS) - a training-free framework guided by static textual and dynamic textual prompts. This framework achieves coarse grounding of high-probability anomalous regions without requiring anomaly-specific prompts, followed by fine-grained anomaly interpretation and temporal boundary refinement. Furthermore, we innovatively propose the Joint evaluation of Anomaly Understanding and Grounding (JeAUG) metric, which overcomes the unidimensional limitations of conventional evaluation systems by jointly assessing semantic interpretability accuracy and temporal grounding precision. Extensive experiments demonstrate the superiority of our proposed benchmark, framework, and evaluation metrics."
                     data-abs-zh="视频异常检测（VAD）旨在识别视频中的异常事件并精确定位其发生时间。目前的VAD研究主要可分为两种范式：传统的基于DNN的方法主要关注异常的时间定位，而利用大语言模型（LLMs）的新兴方法则强调对异常内容的语义理解。事实上，视频异常理解和视频异常定位任务对于视频中异常的综合检测都至关重要。此外，这两个任务可以相互支持和印证。然而，目前还没有模型能够同时完成这两项任务，也没有能够同时支持这两项任务的数据集。在本文中，我们提出了VAGU（视频异常定位与理解），这是首个集成了异常定位和异常理解的基准测试。具体而言，VAGU中的每个实例都标注了三个手动策划的组件：异常类别说明、异常语义理解和精确的异常时间定位。基于该基准，我们提出了Glance then Scrutinize (GtS)——一个由静态文本和动态文本提示引导的免训练框架。该框架无需异常特定提示即可实现高概率异常区域的粗略定位，随后进行细粒度的异常解释和时间边界细化。此外，我们创新性地提出了异常理解与定位联合评估（JeAUG）指标，通过联合评估语义可解释性准确度和时间定位精度，克服了传统评估系统的一维局限性。广泛的实验证明了我们提出的基准、框架和评估指标的优越性。">
                    VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly Grounding and Understanding
                </div>
                <div class="pub-authors">Shibo Gao, Peipei Yang, <span class="me">Yi Chen</span>, Han Zhu, Yangyang Liu, Wenxin Zhang, Linlin Huang</div>
                <div class="pub-venue">AAAI 2026</div>
            </div>

            <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information"
                     data-title-zh="可恢复压缩：文本信息引导的多模态视觉Token恢复机制"
                     data-abs-en="With the advancement of large-scale language modeling techniques, large multimodal models combining visual encoders with large language models have demonstrated exceptional performance in various visual tasks. Most of the current large-scale multimodal models achieve this by mapping visual features obtained from the visual encoder into a large language model and using them as inputs alongside text for downstream tasks. Therefore, the number of visual tokens directly affects the training and inference speed of the model. There has been significant work on token pruning for visual transformers, but for large multimodal models, only relying on visual information for token pruning or compression may lead to significant loss of important information. On the other hand, the textual input in the form of a question may contain valuable information that can aid in answering the question, providing additional knowledge to the model. To address the potential oversimplification and excessive pruning that can occur with most purely visual token pruning methods, we propose a text information-guided dynamic visual token recovery mechanism that does not require training. This mechanism leverages the similarity between the question text and visual tokens to recover visually meaningful tokens with important text information while merging other less important tokens. Experimental results demonstrate that our proposed method achieves comparable performance to the original approach while compressing the visual tokens to an average of 10% of the original quantity."
                     data-abs-zh="随着大规模语言建模技术的进步，结合视觉编码器和大语言模型的大型多模态模型在各种视觉任务中表现出了卓越的性能。当前大多数大型多模态模型通过将从视觉编码器获得的视觉特征映射到大语言模型中，并将其与文本一起作为下游任务的输入来实现这一点。因此，视觉Token的数量直接影响模型的训练和推理速度。虽然在视觉Transformer的Token剪枝方面已有大量工作，但对于大型多模态模型，仅依赖视觉信息进行Token剪枝或压缩可能会导致重要信息的严重丢失。另一方面，以问题形式出现的文本输入可能包含有助于回答问题的有价值信息，为模型提供额外的知识。为解决大多数纯视觉Token剪枝方法可能导致的过度简化和过度剪枝问题，我们提出了一种无需训练的文本信息引导动态视觉Token恢复机制。该机制利用问题文本与视觉Token之间的相似性，恢复具有重要文本信息的视觉上有意义的Token，同时合并其他不太重要的Token。实验结果表明，我们提出的方法在将视觉Token压缩至原始数量平均10%的同时，实现了与原始方法相当的性能。">
                    Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information
                </div>
                <div class="pub-authors"><span class="me">Yi Chen</span>, Jian Xu, Xu-Yao Zhang, Wen-Zhuo Liu, Yang-Yang Liu, Cheng-Lin Liu</div>
                <div class="pub-venue">AAAI 2025</div>
                <div class="pub-links">
                    <a href="https://github.com/banjiuyufen/Recoverable-Compression" class="pub-link"><i class="fab fa-github"></i> Code</a>
                </div>
            </div>

            <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="Decoupling Layout from Glyph in Online Chinese Handwriting Generation"
                     data-title-zh="在线中文手写生成中的字形与布局解耦"
                     data-abs-en="Text plays a crucial role in the transmission of human civilization, and teaching machines to generate online handwritten text in various styles presents an interesting and significant challenge. However, most prior work has concentrated on generating individual Chinese fonts, leaving complete text line generation largely unexplored. In this paper, we identify that text lines can naturally be divided into two components: layout and glyphs. Based on this division, we designed a text line layout generator coupled with a diffusion-based stylized font synthesizer to address this challenge hierarchically. More concretely, the layout generator performs in-context-like learning based on the text content and the provided style references to generate positions for each glyph autoregressively. Meanwhile, the font synthesizer which consists of a character embedding dictionary, a multi-scale calligraphy style encoder and a 1D U-Net based diffusion denoiser will generate each font on its position while imitating the calligraphy style extracted from the given style references. Qualitative and quantitative experiments on the CASIA-OLHWDB demonstrate that our method is capable of generating structurally correct and indistinguishable imitation samples."
                     data-abs-zh="文字在人类文明的传承中起着至关重要的作用，教机器生成各种风格的在线手写文本是一个有趣且重大的挑战。然而，之前的大多数工作都集中在生成单个中文字体上，而在很大程度上未探索完整的文本行生成。在本文中，我们发现文本行可以自然地分为两个部分：布局和字形。基于这种划分，我们设计了一个与基于扩散的风格化字体合成器相耦合的文本行布局生成器，以分层解决这一挑战。具体而言，布局生成器基于文本内容和提供的风格参考执行类上下文学习，以自回归方式生成每个字形的位置。同时，由字符嵌入字典、多尺度书法风格编码器和基于1D U-Net的扩散去噪器组成的字体合成器将在其位置生成每个字体，同时模仿从给定风格参考中提取的书法风格。在CASIA-OLHWDB上的定性和定量实验表明，我们的方法能够生成结构正确且难以区分的模仿样本。">
                    Decoupling Layout from Glyph in Online Chinese Handwriting Generation
                </div>
                <div class="pub-authors">Min-Si Ren, Yan-Ming Zhang, <span class="me">Yi Chen</span></div>
                <div class="pub-venue">ICLR 2025</div>
                <div class="pub-links">
                    <a href="https://github.com/singularityrms/OLHWG" class="pub-link"><i class="fab fa-github"></i> Code</a>
                </div>
            </div>

            <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="Recognition of Online Handwritten Chinese Texts in Any Writing Direction via Stroke Classification Based Over-Segmentation"
                     data-title-zh="基于笔画分类过切分的任意方向在线手写中文文本识别"
                     data-abs-en="Online handwritten text recognition technology has been increasingly applied in intelligent touch-based and pen-based devices. Current mainstream methods are mostly designed for horizontally written texts, thus is difficult to handle texts in any writing direction. This paper proposes a recognition framework based on over-segmentation which is applicable to text recognition of any writing direction. It divides text line inclination styles into two cases: texts with the entire line rotated and texts with the line direction rotated while keeping the characters upright. A text line inclination style classification module is introduced in the preprocessing stage to classify these two cases. The former case can be recognized using a horizontal text line recognizer after rotation correction. For the latter case, an improved over-segmentation algorithm is designed based on stroke classification using bidirectional long short-term memory networks (BiLSTM) to achieve text recognition in any writing direction. Experimental results demonstrate that the proposed method is capable of text recognition in any writing direction and achieves highly competitive results on the CASIA-OLHWDB and ICDAR2013-Online datasets."
                     data-abs-zh="在线手写文本识别技术已越来越多地应用于智能触控和笔式设备中。当前的主流方法大多是针对横向书写的文本设计的，因此难以处理任意书写方向的文本。本文提出了一种基于过切分的识别框架，适用于任意书写方向的文本识别。它将文本行倾斜样式分为两种情况：整行旋转的文本和行方向旋转但字符保持直立的文本。在预处理阶段引入了文本行倾斜样式分类模块来对这两种情况进行分类。对于前一种情况，可以在旋转校正后使用水平文本行识别器进行识别。对于后一种情况，设计了一种基于双向长短期记忆网络（BiLSTM）笔画分类的改进过切分算法，以实现任意书写方向的文本识别。实验结果表明，所提出的方法能够识别任意书写方向的文本，并在CASIA-OLHWDB和ICDAR2013-Online数据集上取得了极具竞争力的结果。">
                     Recognition of Online Handwritten Chinese Texts in Any Writing Direction
                </div>
                <div class="pub-authors"><span class="me">Yi Chen</span>, Heng Zhang, Min-Si Ren, Cheng-Lin Liu</div>
                <div class="pub-venue">ICPR 2024</div>
            </div>
            
             <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="Context-Aware Confidence Estimation for Rejection in Handwritten Chinese Text Recognition"
                     data-title-zh="手写中文文本识别中用于拒识的上下文感知置信度估计"
                     data-abs-en="Handwritten Chinese Text Recognition (HCTR) has been advanced largely by deep learning in recent years. However, the remaining recognition errors still hinder reliability-critical applications where zero-error is desired. Rejecting low-confidence patterns can help reduce the error rate but the increased rejection rate is also harmful. In this paper, we propose a character confidence estimation method incorporating contexts for character rejection in HCTR. Based on a text line recognizer outputting character segmentation and classification results, the confidence of each segmented character is estimated by combining the scores of a re-trained character classifier, the linguistic and geometric contexts. We introduce a probabilistic formula for estimating the confidence by combining the classifier and contextual scores, and an improved approach for scoring the geometric context using unary and binary geometric features. Experimental evaluations on the CASIA-HWDB and ICDAR2013 datasets demonstrate that our method can significantly improve the rejection performance in respect of low error rate at moderate rejection rate. The re-trained classifier, the linguistic context and the geometric context are all justified effective to improve the confidence."
                     data-abs-zh="近年来，深度学习极大地推动了手写中文文本识别（HCTR）的发展。然而，剩余的识别错误仍然阻碍了要求零错误的可靠性关键应用。拒绝低置信度模式有助于降低错误率，但增加的拒识率也是有害的。在本文中，我们提出了一种结合上下文的字符置信度估计方法，用于HCTR中的字符拒识。基于输出字符切分和分类结果的文本行识别器，通过结合重训练的字符分类器分数、语言上下文和几何上下文来估计每个切分字符的置信度。我们引入了一个概率公式，通过结合分类器和上下文分数来估计置信度，以及一种使用一元和二元几何特征对几何上下文进行评分的改进方法。在CASIA-HWDB和ICDAR2013数据集上的实验评估表明，我们的方法可以在中等拒识率下显著提高低错误率方面的拒识性能。重训练的分类器、语言上下文和几何上下文都被证明能有效提高置信度。">
                     Context-Aware Confidence Estimation for Rejection in Handwritten Chinese Text Recognition
                </div>
                <div class="pub-authors">Yang-Yang Liu, <span class="me">Yi Chen</span>, Fei Yin, Cheng-Lin Liu</div>
                <div class="pub-venue">ICDAR 2024</div>
            </div>

             <div class="pub-item">
                <div class="pub-title lang-pub"
                     data-title-en="Improved Learning for Online Handwritten Chinese Text Recognition with Convolutional Prototype Network"
                     data-title-zh="基于卷积原型网络的在线手写中文文本识别改进学习方法"
                     data-abs-en="Segmentation-based handwritten text recognition has the advantage of character interpretability but needs a character classifier with high classification accuracy and non-character rejection capability. The classifier can be trained on both character samples and string samples but real string samples are usually insufficient. In this paper, we proposed a learning method for segmentation-based online handwritten Chinese text recognition with a convolutional prototype network as the underlying classifier. The prototype classifier is inherently resistant to non-characters, and so, can be trained with character and string samples without the need of data augmentation. The learning has two stages: pre-training on character samples with a modified loss function for improving non-character resistance, and weakly supervised learning on both character and string samples for improving recognition performance. Experimental results on the CASIA-OLHWDB and ICDAR2013-Online datasets show that the proposed method can achieve promising recognition performance without training data augmentation."
                     data-abs-zh="基于切分的手写文本识别具有字符可解释性的优势，但需要一个具有高分类精度和非字符拒识能力的字符分类器。分类器可以在字符样本和字符串样本上进行训练，但真实的字符串样本通常不足。在本文中，我们提出了一种以卷积原型网络为底层分类器的基于切分的在线手写中文文本识别学习方法。原型分类器天生具有抗非字符能力，因此，无需数据增强即可使用字符和字符串样本进行训练。学习分为两个阶段：在字符样本上使用改进的损失函数进行预训练以提高抗非字符能力，以及在字符和字符串样本上进行弱监督学习以提高识别性能。在CASIA-OLHWDB和ICDAR2013-Online数据集上的实验结果表明，所提出的方法无需训练数据增强即可实现令人满意的识别性能。">
                     Improved Learning for Online Handwritten Chinese Text Recognition with Convolutional Prototype Network
                </div>
                <div class="pub-authors"><span class="me">Yi Chen</span>, Heng Zhang, Cheng-Lin Liu</div>
                <div class="pub-venue">ICDAR 2023</div>
            </div>
        </div>

        <footer>
            &copy; 2025 Yi Chen. <br>
            <span style="font-size: 0.8rem; opacity: 0.6;">Last updated: Dec 2025</span>
        </footer>

    </div>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const themeBtn = document.getElementById('themeToggle');
            const langBtn = document.getElementById('langToggle');
            const tooltip = document.getElementById('abstract-tooltip');
            const tooltipContent = document.getElementById('tooltip-content');
            
            let lang = 'en'; // Default Language

            // --- Theme Logic ---
            themeBtn.addEventListener('click', () => {
                document.documentElement.classList.toggle('dark-mode');
                const isDark = document.documentElement.classList.contains('dark-mode');
                themeBtn.innerHTML = isDark ? '<i class="fas fa-sun"></i>' : '<i class="fas fa-moon"></i>';
            });

            // --- Language Logic ---
            langBtn.addEventListener('click', () => {
                lang = lang === 'en' ? 'zh' : 'en';
                langBtn.querySelector('span').textContent = lang === 'en' ? 'CN' : 'EN';
                updateLanguage();
            });

            function updateLanguage() {
                // Update simple text elements
                document.querySelectorAll('.lang-text').forEach(el => {
                    const newText = el.getAttribute(`data-${lang}`);
                    if(newText) el.innerHTML = newText;
                });

                // Update Publication Titles
                document.querySelectorAll('.lang-pub').forEach(el => {
                    const newTitle = el.getAttribute(`data-title-${lang}`);
                    if(newTitle) el.textContent = newTitle;
                });
            }

            // --- Tooltip Logic ---
            const titles = document.querySelectorAll('.pub-title');

            titles.forEach(item => {
                item.addEventListener('mouseenter', (e) => {
                    const abs = item.getAttribute(`data-abs-${lang}`);
                    if(!abs) return;
                    
                    tooltipContent.textContent = abs;
                    tooltip.classList.add('active');
                    moveTooltip(e);
                });

                item.addEventListener('mouseleave', () => {
                    tooltip.classList.remove('active');
                });

                item.addEventListener('mousemove', (e) => {
                    moveTooltip(e);
                });
            });

            function moveTooltip(e) {
                const x = e.clientX;
                const y = e.clientY;
                const box = tooltip.getBoundingClientRect();
                
                // Position offset
                let left = x + 25; 
                let top = y + 25;

                // Prevent overflow right
                if (left + box.width > window.innerWidth) {
                    left = x - box.width - 25;
                }
                
                // Prevent overflow bottom
                if (top + box.height > window.innerHeight) {
                    top = y - box.height - 25;
                }
                
                // Ensure top is never negative
                if (top < 10) top = 10;

                tooltip.style.left = `${left}px`;
                tooltip.style.top = `${top}px`;
            }
        });
    </script>
</body>
</html>
