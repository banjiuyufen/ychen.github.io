<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yi Chen - Academic Homepage</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&family=Merriweather:ital,wght@0,300;0,400;0,700;1,400&family=JetBrains+Mono:wght@400&display=swap" rel="stylesheet">
    <style>
        :root {
            /* 学术深蓝配色体系 */
            --primary: #003366; /* Oxford Blue */
            --primary-light: #2c5282;
            --primary-dark: #002244;
            --secondary: #536471; /* Slate */
            
            --bg-body: #fdfdfd;
            --bg-card: #ffffff;
            --border-color: #e1e4e8;
            
            --text-main: #24292f;
            --text-muted: #57606a;
            
            --accent: #d97706; /* Amber */
            
            --font-sans: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
            --font-serif: 'Merriweather', Georgia, 'Times New Roman', serif;
            --font-mono: 'JetBrains Mono', Consolas, monospace;

            --border-radius: 6px;
            --shadow-card: 0 1px 3px rgba(0,0,0,0.08), 0 4px 12px rgba(0,0,0,0.02);
            --shadow-hover: 0 8px 24px rgba(0,0,0,0.08);
            --transition: all 0.25s cubic-bezier(0.4, 0, 0.2, 1);
        }
        
        :root.dark-mode {
            --primary: #58a6ff;
            --primary-light: #79c0ff;
            --primary-dark: #1f6feb;
            --secondary: #8b949e;
            
            --bg-body: #0d1117;
            --bg-card: #161b22;
            --border-color: #30363d;
            
            --text-main: #c9d1d9;
            --text-muted: #8b949e;
            
            --accent: #d29922;
            
            --shadow-card: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-hover: 0 8px 24px rgba(0,0,0,0.4);
        }
        
        * { margin: 0; padding: 0; box-sizing: border-box; }
        
        body {
            font-family: var(--font-sans);
            line-height: 1.7;
            color: var(--text-main);
            background-color: var(--bg-body);
            background-image: radial-gradient(#e5e7eb 1px, transparent 1px);
            background-size: 20px 20px;
            padding: 40px 20px;
            min-height: 100vh;
            transition: background 0.3s ease, color 0.3s ease;
        }
        
        :root.dark-mode body { background-image: radial-gradient(#21262d 1px, transparent 1px); }

        .container { max-width: 1080px; margin: 0 auto; position: relative; z-index: 2; }
        
        .header { text-align: left; padding: 60px 0; margin-bottom: 20px; border-bottom: 1px solid var(--border-color); }
        .header h1 { font-family: var(--font-serif); font-size: 3.5rem; font-weight: 700; color: var(--text-main); margin-bottom: 10px; letter-spacing: -0.02em; }
        .header p { font-size: 1.25rem; color: var(--text-muted); font-weight: 300; max-width: 800px; }

        .grid { display: grid; gap: 30px; }
        .card { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: var(--border-radius); padding: 35px; box-shadow: var(--shadow-card); transition: var(--transition); }
        .card:hover { box-shadow: var(--shadow-hover); border-color: var(--primary-light); }
        
        .section-title { font-family: var(--font-sans); font-size: 1.5rem; font-weight: 600; color: var(--text-main); margin-bottom: 25px; padding-bottom: 12px; border-bottom: 2px solid var(--primary); display: flex; align-items: center; gap: 12px; letter-spacing: -0.01em; }
        .section-title i { font-size: 1.2rem; color: var(--primary); opacity: 0.8; }

        .profile { display: flex; gap: 40px; align-items: flex-start; }
        .profile-img { width: 180px; height: 180px; border-radius: 50%; object-fit: cover; border: 3px solid var(--bg-card); box-shadow: 0 4px 10px rgba(0,0,0,0.1); flex-shrink: 0; }
        
        a { color: var(--primary); text-decoration: none; transition: color 0.2s; }
        a:hover { text-decoration: underline; color: var(--primary-light); }

        .social-links { display: flex; flex-wrap: wrap; gap: 12px; margin: 20px 0; }
        .social-link { font-size: 0.9rem; padding: 6px 14px; border-radius: 4px; background: transparent; border: 1px solid var(--border-color); color: var(--text-muted); display: flex; align-items: center; gap: 8px; }
        .social-link:hover { background: var(--primary); color: #fff; border-color: var(--primary); text-decoration: none; }

        .contact-info { font-size: 0.95rem; color: var(--text-muted); display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 10px; margin-top: 20px; padding-top: 20px; border-top: 1px dashed var(--border-color); }
        .contact-row { display: flex; align-items: center; gap: 10px; }
        .contact-row i { color: var(--primary); width: 16px; }

        .research-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 30px; }
        .research-interests li { margin-bottom: 10px; list-style-type: none; padding-left: 20px; position: relative; }
        .research-interests li::before { content: "▹"; position: absolute; left: 0; color: var(--primary); }

        .highlight { color: var(--accent); font-weight: 600; background: rgba(217, 119, 6, 0.1); padding: 0 4px; border-radius: 3px; }
        .note { margin-top: 20px; padding: 15px; background: rgba(0, 51, 102, 0.04); border-left: 3px solid var(--primary); font-size: 0.95rem; font-style: italic; color: var(--text-muted); }
        :root.dark-mode .note { background: rgba(88, 166, 255, 0.1); }

        .publication-list { list-style: none; }
        .publication-list li { margin-bottom: 30px; position: relative; padding-left: 20px; }
        
        .publication-title { 
            font-family: var(--font-serif); 
            font-size: 1.15rem; 
            font-weight: 700; 
            color: var(--text-main); 
            margin-bottom: 6px; 
            cursor: pointer; 
            line-height: 1.4; 
            position: relative; 
            display: inline-block;
            transition: color 0.2s;
        }
        .publication-title:hover { color: var(--primary); }
        .publication-title::after {
            content: '';
            position: absolute;
            width: 100%;
            height: 2px;
            bottom: -2px;
            left: 0;
            background-image: linear-gradient(to right, var(--text-muted) 33%, rgba(255,255,255,0) 0%);
            background-position: bottom;
            background-size: 5px 1px;
            background-repeat: repeat-x;
            opacity: 0.5;
        }
        
        .publication-authors { font-size: 1rem; color: var(--text-muted); margin-bottom: 6px; }
        .publication-venue { font-size: 0.95rem; font-style: italic; color: var(--primary); margin-bottom: 10px; }
        .publication-links a { font-size: 0.85rem; font-family: var(--font-mono); text-transform: uppercase; margin-right: 15px; color: var(--text-muted); font-weight: 600; }
        .publication-links a:hover { color: var(--primary); }

        .academic-activities { display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 20px; }
        .activity-item, .contribution-item { background: var(--bg-body); border: 1px solid var(--border-color); border-radius: var(--border-radius); padding: 20px; transition: var(--transition); }
        .activity-item:hover, .contribution-item:hover { transform: translateY(-3px); border-color: var(--primary); }
        .activity-title, .contribution-title { font-weight: 600; font-size: 1.05rem; margin-bottom: 8px; display: flex; justify-content: space-between; align-items: flex-start; }
        .activity-year { font-family: var(--font-mono); font-size: 0.8rem; background: var(--border-color); padding: 2px 8px; border-radius: 4px; color: var(--text-main); }

        .tech-stack { margin-top: 15px; display: flex; gap: 8px; flex-wrap: wrap; }
        .tech-item { font-size: 0.75rem; font-family: var(--font-mono); background: rgba(0,0,0,0.05); padding: 2px 8px; border-radius: 3px; color: var(--text-muted); }
        :root.dark-mode .tech-item { background: rgba(255,255,255,0.1); }

        .control-bar { position: absolute; top: 60px; right: 0; display: flex; gap: 10px; }
        .control-btn { background: transparent; border: 1px solid var(--border-color); color: var(--text-muted); width: 36px; height: 36px; border-radius: 50%; cursor: pointer; display: flex; align-items: center; justify-content: center; transition: all 0.2s; }
        .control-btn:hover { background: var(--bg-card); color: var(--primary); border-color: var(--primary); box-shadow: var(--shadow-card); }

        .footer { text-align: center; margin-top: 60px; padding-top: 30px; border-top: 1px solid var(--border-color); font-size: 0.9rem; color: var(--text-muted); }

        /* --- Tooltip Styles --- */
        .tooltip-container {
            display: none; 
            position: fixed; 
            background: var(--bg-card);
            border: 1px solid var(--border-color);
            padding: 20px 25px;
            width: 600px;
            max-width: 90vw;
            border-radius: var(--border-radius);
            box-shadow: 0 10px 40px rgba(0,0,0,0.2);
            z-index: 10000; 
            font-size: 0.95rem;
            line-height: 1.6;
            text-align: justify;
            color: var(--text-main);
            pointer-events: none; 
        }
        
        .tooltip-container.visible {
            display: block;
            animation: fadeIn 0.2s ease-out;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(5px); }
            to { opacity: 1; transform: translateY(0); }
        }
        
        :root.dark-mode .tooltip-container {
            box-shadow: 0 10px 40px rgba(0,0,0,0.6);
            border: 1px solid var(--border-color);
        }

        .tooltip-label {
            font-family: var(--font-mono);
            font-size: 0.75rem;
            text-transform: uppercase;
            color: var(--primary);
            margin-bottom: 8px;
            display: block;
            font-weight: 600;
            letter-spacing: 0.5px;
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 4px;
        }

        @media (max-width: 768px) {
            .profile { flex-direction: column; align-items: center; text-align: center; }
            .header h1 { font-size: 2.5rem; }
            .research-grid { grid-template-columns: 1fr; }
            .contact-info { justify-content: center; }
            .control-bar { position: relative; top: 0; justify-content: flex-end; margin-bottom: 20px; }
            .header { padding-top: 20px; }
            .tooltip-container { display: none !important; } 
        }
    </style>
</head>
<body>
    
    <div id="abstract-tooltip" class="tooltip-container">
        <span class="tooltip-label">Abstract</span>
        <div id="tooltip-content"></div>
    </div>

    <div class="container">
        <div class="control-bar">
            <button class="control-btn theme-toggle" id="themeToggle" title="Switch Theme">
                <i class="fas fa-moon"></i>
            </button>
            <button class="control-btn lang-toggle" id="langToggle" title="Switch Language">
                <span style="font-size: 0.8rem; font-weight: bold;">EN</span>
            </button>
        </div>

        <div class="header">
            <h1>Yi Chen</h1>
            <p class="lang-switch" data-lang-en="PhD Candidate in Pattern Recognition and Intelligent Systems" data-lang-zh="模式识别与智能系统专业 · 博士候选人">
                PhD Candidate in Pattern Recognition and Intelligent Systems
            </p>
        </div>
        
        <div class="grid">
            <div class="card">
                <div class="profile">
                    <img src="ychen.jpg" alt="Yi Chen" class="profile-img">
                    <div class="profile-info">
                        <h2 class="section-title">
                            <i class="fas fa-user-circle"></i>
                            <span class="lang-switch" data-lang-en="Personal Profile" data-lang-zh="个人简介">Personal Profile</span>
                        </h2>
                        <p class="lang-switch" data-lang-en="I am currently a PhD student jointly affiliated with the State Key Laboratory of Multimodal Artificial Intelligence Systems at the Institute of Automation, Chinese Academy of Sciences (CASIA) and Zhongguancun Academy, under the supervision of Prof. <a href='https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN'>Cheng-Lin Liu</a>." data-lang-zh="本人目前是中国科学院自动化研究所多模态人工智能系统国家重点实验室（MAIS）与中关村学院联合培养的博士研究生，师从<a href='https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN'>刘成林</a>研究员。">
                            I am currently a PhD student jointly affiliated with the State Key Laboratory of Multimodal Artificial Intelligence Systems at the Institute of Automation, Chinese Academy of Sciences (CASIA) and Zhongguancun Academy, under the supervision of Prof. <a href="https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN">Cheng-Lin Liu</a>.
                        </p>
                        <p class="lang-switch" data-lang-en="I received my B.E. degree from the School of Space Science and Technology at Xidian University in 2021. Subsequently, I earned my Master's degree in Electronic Information from the National Laboratory of Pattern Recognition (NLPR), CASIA in 2024, also advised by Prof. <a href='https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN'>Cheng-Lin Liu</a>." data-lang-zh="本人于2021年在西安电子科技大学空间科学与技术学院获得工学学士学位；2024年在中国科学院自动化研究所模式识别国家重点实验室（NLPR）获得电子信息硕士学位，导师同样为<a href='https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN'>刘成林</a>研究员。">
                            I received my B.E. degree from the School of Space Science and Technology at Xidian University in 2021. Subsequently, I earned my Master's degree in Electronic Information from the National Laboratory of Pattern Recognition (NLPR), CASIA in 2024, also advised by Prof. <a href='https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN'>Cheng-Lin Liu</a>.
                        </p>
                        
                        <div class="social-links">
                            <a href="https://scholar.google.com/citations?user=XoiT9wMAAAAJ&hl=zh-CN" class="social-link"><i class="fas fa-graduation-cap"></i> Google Scholar</a>
                            <a href="https://orcid.org/0009-0005-0720-6372" class="social-link"><i class="fab fa-orcid"></i> ORCID</a>
                            <a href="https://www.researchgate.net/profile/Yi-Chen-287" class="social-link"><i class="fab fa-researchgate"></i> ResearchGate</a>
                            <a href="https://github.com/banjiuyufen" class="social-link"><i class="fab fa-github"></i> GitHub</a>
                        </div>
                        
                        <div class="contact-info">
                            <div class="contact-row">
                                <i class="fas fa-building"></i>
                                <span><a href="https://mais.ia.ac.cn/about/ds.html">CASIA-MAIS / NLPR-PAL Group</a></span>
                            </div>
                            <div class="contact-row">
                                <i class="fas fa-map-marker-alt"></i>
                                <span>Beijing 100190, China</span>
                            </div>
                            <div class="contact-row">
                                <i class="fas fa-envelope"></i>
                                <span>yi.chen@nlpr.ia.ac.cn</span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="research-grid">
                <div class="card">
                    <h2 class="section-title">
                        <i class="fas fa-microscope"></i>
                        <span class="lang-switch" data-lang-en="Research Focus" data-lang-zh="研究方向">Research Focus</span>
                    </h2>
                    <p class="lang-switch" data-lang-en="My Ph.D. research focuses on artificial intelligence and deep learning methodologies, specifically at the intersection of Large Language Models (LLMs) and Adjuvant Science (<span class='highlight'>AI for Adjuvant Discovery</span>). Additionally, I investigate the fundamental theories of Multimodal Large Language Models (MLLMs), including reliable reasoning and inference acceleration." data-lang-zh="本人的博士研究课题专注于人工智能与深度学习方法论，特别是大型语言模型与佐剂学的交叉领域研究（<span class='highlight'>AI佐剂发现</span>）。此外，本人亦致力于多模态大模型（MLLMs）的基础理论研究，涵盖可靠推理与推理加速等前沿方向。">
                        My Ph.D. research focuses on artificial intelligence and deep learning methodologies, specifically at the intersection of Large Language Models (LLMs) and Adjuvant Science (<span class="highlight">AI for Adjuvant Discovery</span>). Additionally, I investigate the fundamental theories of Multimodal Large Language Models (MLLMs), including reliable reasoning and inference acceleration.
                    </p>
                    
                    <div class="note">
                        <span class="lang-switch" data-lang-en="Bridging AI theory with practical applications in scientific domains." data-lang-zh="致力于将人工智能理论与科学领域的实际应用相结合，推动AI for Science的发展。">
                            Bridging AI theory with practical applications in scientific domains.
                        </span>
                    </div>
                </div>
                
                <div class="card">
                    <h2 class="section-title">
                        <i class="fas fa-star"></i>
                        <span class="lang-switch" data-lang-en="Research Interests" data-lang-zh="研究兴趣">Research Interests</span>
                    </h2>
                    <ul class="research-interests">
                        <li class="lang-switch" data-lang-en="Recognition and generation of Online Handwritten Chinese Text" data-lang-zh="联机中文手写文本行的识别与生成">
                            Recognition and generation of Online Handwritten Chinese Text
                        </li>
                        <li class="lang-switch" data-lang-en="Foundations and applications of LLMs and MLLMs" data-lang-zh="大语言模型与多模态基座模型的基础理论与应用">
                            Foundations and applications of LLMs and MLLMs
                        </li>
                        <li class="lang-switch" data-lang-en="AI for Science (Biology, Chemistry, Materials Science)" data-lang-zh="AI for Science（生物、化学及材料科学方向）">
                            AI for Science (Biology, Chemistry, Materials Science)
                        </li>
                    </ul>
                    <div style="margin-top: 20px; font-size: 0.9rem; color: var(--primary);">
                         <span class="lang-switch" data-lang-en="Open to collaboration. Please feel free to contact via email." data-lang-zh="诚挚欢迎学术交流与合作，请通过邮件联系。">
                            Open to collaboration. Please feel free to contact via email.
                        </span>
                    </div>
                </div>
            </div>
            
            <div class="card">
                <h2 class="section-title">
                    <i class="fas fa-code-branch"></i>
                    <span class="lang-switch" data-lang-en="Community Contributions" data-lang-zh="开源与社区贡献">Community Contributions</span>
                </h2>
                
                <div class="contribution-item">
                    <h3 class="contribution-title">
                        <span class="lang-switch" data-lang-en="PaddleScience Contributor" data-lang-zh="PaddleScience 核心贡献">PaddleScience Contributor</span>
                        <a href="https://github.com/PaddlePaddle/PaddleScience/pull/977" style="font-size: 0.9rem;"><i class="fas fa-link"></i> PR #977</a>
                    </h3>
                    
                    <p class="lang-switch" data-lang-en="Integrated the <strong><a href='https://doi.org/10.1103/PhysRevLett.120.145301'>Crystal Graph CNN (CGCNN)</a></strong> model into the <strong>PaddleScience</strong> toolkit for materials chemistry applications." data-lang-zh="成功将 <strong><a href='https://doi.org/10.1103/PhysRevLett.120.145301'>Crystal Graph CNN (CGCNN)</a></strong> 模型集成至百度飞桨科学计算工具包 <strong>PaddleScience</strong> 中，用于材料化学领域的应用。">
                        Integrated the <strong><a href='https://doi.org/10.1103/PhysRevLett.120.145301'>Crystal Graph CNN (CGCNN)</a></strong> model into the <strong>PaddleScience</strong> toolkit for materials chemistry applications.
                    </p>
                    
                    <ul class="research-interests" style="margin-top: 10px; font-size: 0.95rem;">
                        <li class="lang-switch" data-lang-en="Implemented full pipeline: preprocessing, graph construction, training, and inference." data-lang-zh="完整实现了晶体结构数据预处理、图神经网络构建、训练及推理的全流程。">
                            Implemented full pipeline: preprocessing, graph construction, training, and inference.
                        </li>
                        <li class="lang-switch" data-lang-en="Code merged into official repository and featured as an official case study." data-lang-zh="代码已合并至官方主仓库，并被收录为官方材料化学应用案例。">
                            Code merged into official repository and featured as an official case study.
                        </li>
                    </ul>
                    
                    <div class="tech-stack">
                        <span class="tech-item">PaddlePaddle</span>
                        <span class="tech-item">GNN</span>
                        <span class="tech-item">Materials Informatics</span>
                    </div>
                </div>
            </div>
            
            <div class="card">
                <h2 class="section-title">
                    <i class="fas fa-user-tie"></i>
                    <span class="lang-switch" data-lang-en="Academic Service" data-lang-zh="学术服务">Academic Service</span>
                </h2>
                
                <div class="academic-activities">
                    <div class="activity-item">
                        <div class="activity-title">
                            AAAI 2026
                            <span class="activity-year">PC Member</span>
                        </div>
                        <div class="lang-switch" data-lang-en="Program Committee Member (Main Track)" data-lang-zh="主会程序委员会委员">
                            Program Committee Member (Main Track)
                        </div>
                    </div>

                    <div class="activity-item">
                        <div class="activity-title">
                            ICLR 2026
                            <span class="activity-year">Reviewer</span>
                        </div>
                        <div class="lang-switch" data-lang-en="Main Track Reviewer" data-lang-zh="主会审稿人">
                            Main Track Reviewer
                        </div>
                    </div>

                    <div class="activity-item">
                        <div class="activity-title">
                            CVPR 2026
                            <span class="activity-year">Reviewer</span>
                        </div>
                        <div class="lang-switch" data-lang-en="Main Track Reviewer" data-lang-zh="主会审稿人">
                            Main Track Reviewer
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="card">
                <h2 class="section-title">
                    <i class="fas fa-file-contract"></i>
                    <span class="lang-switch" data-lang-en="Preprints" data-lang-zh="预印本论文">Preprints</span>
                </h2>
                <p style="font-size: 0.9rem; color: var(--text-muted); margin-bottom: 20px;">
                    <i class="lang-switch" data-lang-en="(*: equal contribution)" data-lang-zh="(*: 同等贡献)">(*: equal contribution)</i>
                </p>
                
                <ul class="publication-list">
                    <li>
                        <div class="publication-title lang-switch" 
                             data-lang-en="An Open-Ended Benchmark and Formal Framework for Adjuvant Research with MLLM" 
                             data-lang-zh="面向大模型佐剂研究的开放式基准测试与形式化框架"
                             data-abstract-en="Adjuvants play a critical role in modulating immune responses and are central to the development of vaccines and immunotherapies. Yet progress in this field is constrained by data scarcity and incomplete understanding of mechanisms of action, which limit the transition from experience-based design to AI-driven approaches. To address these challenges, we present the first benchmark dedicated to adjuvants, constructed in an open-ended Q&A format and annotated by domain experts. The benchmark comprises 1,294 Q&A pairs and 1,364 formal descriptions, providing a resource for evaluating general-purpose multimodal large language models (MLLMs) and for developing domain-specific systems.  We systematically assess 11 closed-source and 18 open-source MLLMs across dimensions including domain-specific Q&A, hallucination rejection, data generation, and instruction following. Results indicate that OpenAI-o1 (STS = 0.7495, LLM Score = 7.7) and DeepSeek-R1 (STS = 0.7415, LLM Score = 7.7) achieved the strongest performance among closed- and open-source models, respectively. In addition, we introduce a formal description framework for representing adjuvant design principles and immune mechanisms as structured abstractions, which can serve as building blocks for future domain-specialized MLLMs. Overall, this work provides a first step toward systematically integrating MLLMs into adjuvant research by offering a dedicated benchmark, comparative evaluation of existing models, and a formal foundation for future development."
                             data-abstract-zh="佐剂在调节免疫反应中起着至关重要的作用，是疫苗和免疫疗法开发的核心。然而，由于数据匮乏以及对作用机制理解的不完整，该领域的进展受到限制，阻碍了从基于经验的设计向AI驱动方法的转变。为解决这些挑战，我们提出了首个专门针对佐剂的基准测试，采用开放式问答格式构建，并由领域专家进行标注。该基准包含1,294对问答和1,364条形式化描述，为评估通用多模态大语言模型（MLLMs）和开发领域特定系统提供了重要资源。我们从领域特定问答、幻觉拒绝、数据生成和指令遵循等维度系统评估了11个闭源和18个开源MLLMs。结果表明，OpenAI-o1（STS = 0.7495，LLM评分 = 7.7）和DeepSeek-R1（STS = 0.7415，LLM评分 = 7.7）分别在闭源和开源模型中取得了最强性能。此外，我们引入了一个形式化描述框架，将佐剂设计原则和免疫机制表示为结构化抽象，作为未来领域专用MLLMs的构建模块。总体而言，这项工作通过提供专用基准、现有模型的比较评估以及未来发展的形式化基础，迈出了将MLLMs系统整合到佐剂研究中的第一步。">
                            An Open-Ended Benchmark and Formal Framework for Adjuvant Research with MLLM
                        </div>
                        <div class="publication-authors"><span class="highlight">Yi Chen*</span>, Yu Zhang*, Jian Xu, Xu-Yao Zhang, Hua Yue, Xinming Wang, Zequan Lyu, Wei Wei, Cheng-Lin Liu</div>
                        <div class="publication-links">
                            <a href="#"><i class="fas fa-file-pdf"></i> arXiv (Coming Soon)</a>
                        </div>
                    </li>
                    
                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="Sparsity Meets Similarity: Leveraging Long-Tail Distribution for Dynamic Optimized Token Representation in Multimodal Large Language Models"
                             data-lang-zh="稀疏性遇见相似性：利用长尾分布优化多模态大模型的动态Token表示"
                             data-abstract-en="Recently, multimodal large language models (MM-LLMs) have achieved significant success in various tasks, but their high computational costs limit widespread application. The main computational burden arises from processing concatenated text and visual tokens in the LLM layer, where input token length directly affects efficiency. Our analysis of visual tokens reveals that their similarity to the CLS token follows a long-tail distribution, with only a few showing high similarity. To address this, we propose a dynamic pruning algorithm that identifies the inflection point in the visual CLS token similarity curve, enabling effective trimming of visual markers to accelerate model performance. Additionally, we perform a second round of pruning in the LLM layer, filtering out low-correlation tokens through the interaction between visual and textual features. Experimental results demonstrate that our method achieves performance comparable to the original while utilizing only 22% of the original token quantity. Our source code will be made publicly available upon acceptance."
                             data-abstract-zh="近年来，多模态大语言模型（MM-LLMs）在各项任务中取得了显著成功，但高昂的计算成本限制了其广泛应用。主要的计算负担源于LLM层对拼接文本和视觉Token的处理，输入Token长度直接影响效率。我们对视觉Token的分析表明，它们与CLS Token的相似性服从长尾分布，只有少数表现出高相似性。鉴于此，我们提出了一种动态剪枝算法，通过识别视觉CLS Token相似度曲线中的拐点，有效修剪视觉标记以加速模型性能。此外，我们在LLM层进行了第二轮剪枝，通过视觉和文本特征之间的交互过滤掉低相关性的Token。实验结果表明，我们的方法在仅使用22%的原始Token数量的情况下，实现了与原始模型相当的性能。我们的源代码将在论文录用后公开。">
                            Sparsity Meets Similarity: Leveraging Long-Tail Distribution for Dynamic Optimized Token Representation in Multimodal Large Language Models
                        </div>
                        <div class="publication-authors"><span class="highlight">Yi Chen*</span>, Gao-Tong Yu*, Jian Xu</div>
                        <div class="publication-links">
                            <a href="https://arxiv.org/abs/2409.01162"><i class="fas fa-file-pdf"></i> arXiv</a>
                        </div>
                    </li>

                    <li>
                         <div class="publication-title lang-switch"
                              data-lang-en="An Efficient Strategy for Data-constrained Machine Learning in Materials Science"
                              data-lang-zh="材料科学中数据受限场景下的高效机器学习策略"
                              data-abstract-en="Materials science research increasingly benefits from the application of machine learning method, yet encounters fundamental challenges from data scarcity, such as limited dataset sizes and severe data distribution imbalance. In this paper, we develop a multi-task and auxiliary machine learning framework to address these limitations. Using the 2D materials dataset as a case study, our approach demonstrates significantly enhanced prediction accuracy over the baseline crystal graph convolutional neural networks method. Specifically, it reduced the mean absolute error for band gap prediction from 0.30 eV to 0.23 eV and for work function from 0.34 eV to 0.24 eV. The performance gain stems from the framework’s ability to effectively exploit underlying physical correlations between material properties through synergistic multi-task and auxiliary learning. Furthermore, its modular architecture enables seamless integration with various graph based or other end-to-end deep learning models, presenting a computationally efficient and easily implementable solution for constrained datasets in material science."
                              data-abstract-zh="材料科学研究日益受益于机器学习方法的应用，但仍面临数据匮乏带来的根本性挑战，如数据集规模有限和数据分布严重不平衡。为此，我们开发了一种多任务辅助机器学习框架以应对这些限制。以二维材料数据集为例，我们的方法在预测精度上显著优于基线晶体图卷积神经网络方法。具体而言，它将带隙预测的平均绝对误差从0.30 eV降低到0.23 eV，功函数预测的误差从0.34 eV降低到0.24 eV。性能提升源于该框架通过协同多任务和辅助学习有效利用材料属性之间潜在的物理相关性的能力。此外，其模块化架构能够与各种基于图或其他端到端深度学习模型无缝集成，为材料科学中的受限数据集提供了一种计算高效且易于实施的解决方案。">
                            An Efficient Strategy for Data-constrained Machine Learning in Materials Science
                        </div>
                        <div class="publication-authors">ChunTing Shao*, <span class="highlight">Yi Chen*</span>, ShanMan Song, PeiPei Yang, QingBo Yan, Jian Xu, Gang Su</div>
                        <div class="publication-links">
                             <a href="#"><i class="fas fa-file-pdf"></i> arXiv (Coming Soon)</a>
                        </div>
                    </li>

                    <li>
                         <div class="publication-title lang-switch"
                              data-lang-en="ContextRGBNav: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation"
                              data-lang-zh="ContextRGBNav：基于上下文自适应的单目零样本语义导航框架"
                              data-abstract-en="Efficiently finding objects in complex environments is fundamental to real-world embodied applications. While recent advances in multimodal foundation models have enabled zero-shot object goal navigation, allowing robots to search for arbitrary objects without fine-tuning, existing methods face two key limitations: (1) heavy reliance on precise depth and pose information provided by simulators, which restricts applicability in real-world scenarios; and (2) lack of in-context learning (ICL) capability, making it difficult to quickly adapt to new environments from a short video. To address these challenges, we propose ContextRGBNav, a novel zero-shot, open-vocabulary semantic navigation framework that operates using only a monocular camera. Leveraging powerful 3D foundation models, ContextRGBNave liminates the dependency on depth and pose while exhibiting strong ICL capability: by simply observing a short video of a new environment, the system significantly improves task efficiency without requiring architectural modifications or fine-tuning. The framework integrates several key components: keyframe-based 3D reconstruction, semantic point cloud generation, vision-language model (VLM)-driven exploration value estimation, high-level adaptive waypoint selection, and low-level action execution. Experiments on the HM3D benchmark and real-world environments demonstrate that ContextRGBNav achieves competitive performance in terms of navigation success rate and exploration efficiency, while showing superior ICL adaptability."
                              data-abstract-zh="在复杂环境中高效寻找物体是现实世界具身智能应用的基础。尽管多模态基础模型的最新进展实现了零样本目标导航，允许机器人在无需微调的情况下搜索任意物体，但现有方法面临两个主要限制：（1）严重依赖模拟器提供的精确深度和姿态信息，限制了在现实场景中的适用性；（2）缺乏上下文学习（ICL）能力，难以通过短视频快速适应新环境。为解决这些挑战，我们提出了ContextRGBNav，一种新颖的零样本、开放词汇语义导航框架，仅依赖单目相机运行。利用强大的3D基础模型，ContextRGBNav消除了对深度和姿态信息的依赖，同时展现出强大的ICL能力：仅通过观察新环境的短视频，系统即可显著提高任务效率，而无需修改架构或微调。该框架集成了几个关键组件：基于关键帧的3D重建、语义点云生成、视觉语言模型（VLM）驱动的探索价值估计、高级自适应航点选择和低级动作执行。在HM3D基准和现实环境中的实验表明，ContextRGBNav在导航成功率和探索效率方面取得了具有竞争力的性能，同时表现出卓越的ICL适应性。">
                            ContextRGBNav: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation
                        </div>
                        <div class="publication-authors">Ming-Ming Yu, <span class="highlight">Yi Chen</span>, Börje F. Karlsson, Wenjun Wu</div>
                        <div class="publication-links">
                             <a href="#"><i class="fas fa-file-pdf"></i> arXiv (Coming Soon)</a>
                        </div>
                    </li>

                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning"
                             data-lang-zh="ChartAgent：一种基于工具集成推理的图表理解框架"
                             data-abstract-en="With high information density and intuitive readability, charts have become the de facto standard medium for data analysis and communication across disciplines. Although recent multimodal large language models (MLLMs) have achieved notable advances in automated chart understanding, they still exhibit a pronounced dependence on explicit textual annotations; when key numerals are absent, performance degrades markedly. To overcome this limitation, we propose ChartAgent, a chart understanding framework built upon Tool Integrated Reasoning (TIR). Inspired by human cognitive processes, ChartAgent decomposes complex chart-analysis tasks into a sequence of observable and replayable steps. To support this architecture, we develop an extensible, modular visual tool library that encapsulates more than a dozen core tools—including key element detection, instance segmentation, and optical character recognition (OCR)—which the agent dynamically orchestrates to achieve systematic visual parsing across diverse chart types. Leveraging the transparency and verifiability of TIR, ChartAgent moves the decision process beyond the black box paradigm: intermediate outputs from individual tools are standardized and consolidated into an Evidence Package, providing traceable and reproducible support for final conclusions. Experimental results demonstrate that ChartAgent significantly improves chart understanding robustness under sparse-annotation settings, offering a practical pathway toward trustworthy and reliable systems for data-visualization analytics."
                             data-abstract-zh="凭借高信息密度和直观的可读性，图表已成为跨学科数据分析和交流的事实标准媒介。尽管多模态大语言模型（MLLMs）在图表自动理解方面取得了显著进展，但仍明显依赖显式的文本标注；当关键数字缺失时，性能会显著下降。为克服这一局限，我们提出了ChartAgent，一种基于工具集成推理（TIR）的图表理解框架。受人类认知过程启发，ChartAgent将复杂的图表分析任务分解为一系列可观察且可复现的步骤。为支持该架构，我们开发了一个可扩展的模块化视觉工具库，封装了十多种核心工具——包括关键元素检测、实例分割和光学字符识别（OCR）——智能体可动态编排这些工具，以实现对不同图表类型的系统化视觉解析。利用TIR的透明性和可验证性，ChartAgent将决策过程超越了黑箱范式：单个工具的中间输出被标准化并整合为证据包（Evidence Package），为最终结论提供可追溯和可复现的支持。实验结果表明，ChartAgent显著提高了稀疏标注设置下图表理解的鲁棒性，为数据可视化分析的可信赖系统提供了一条切实可行的途径。">
                           ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning
                       </div>
                       <div class="publication-authors">Boran Wang, Xinming Wang, <span class="highlight">Yi Chen</span>, Xiang Li, Jian Xu, Jing Yuan, Cheng-Lin Liu</div>
                       <div class="publication-links">
                            <a href="#"><i class="fas fa-file-pdf"></i> arXiv (Coming Soon)</a>
                       </div>
                   </li>

                   <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="MR-ALIGN: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models"
                             data-lang-zh="MR-ALIGN：基于元推理信息的推理大模型事实性对齐"
                             data-abstract-en="Large reasoning models (LRMs) show strong capabilities in complex reasoning, yet their marginal gains on evidence-dependent factual questions are limited. We find this limitation is partially attributable to a reasoning–answer hit gap, where the model identifies the correct facts during reasoning but fails to incorporate them into the final response, thereby reducing factual fidelity. To address this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment framework that enhances factuality without relying on external verifiers. MR-ALIGN quantifies state-transition probabilities along the model’s thinking process and constructs a transition-aware implicit reward that reinforces beneficial reasoning patterns while suppressing defective ones at the atomic thinking segments. This re-weighting reshapes token-level signals into probability-aware segment scores, encouraging coherent reasoning trajectories that are more conducive to factual correctness. Empirical evaluations across four factual QA datasets and one long-form factuality benchmark show that MR-ALIGN consistently improves accuracy and truthfulness while reducing misleading reasoning. These results highlight that aligning the reasoning process itself, rather than merely the outputs, is pivotal for advancing factuality in LRMs."
                             data-abstract-zh="大推理模型（LRMs）在复杂推理任务中表现出强大的能力，但在依赖证据的事实性问题上增益有限。我们发现这一限制部分归因于推理-答案命中差距，即模型在推理过程中识别出了正确事实，但未能将其纳入最终回答，从而降低了事实保真度。为解决这一问题，我们提出了MR-ALIGN，一种基于元推理信息的对齐框架，无需依赖外部验证器即可增强事实性。MR-ALIGN量化了模型思维过程中的状态转移概率，并构建了一个感知转移的隐式奖励，在原子思维片段层面强化有益的推理模式并抑制有缺陷的模式。这种重加权将Token级信号重塑为感知概率的片段分数，鼓励更有利于事实正确性的连贯推理轨迹。在四个事实性问答数据集和一个长文本事实性基准上的实证评估表明，MR-ALIGN在提高准确性和真实性的同时，一致地减少了误导性推理。这些结果凸显了对齐推理过程本身（而非仅仅是对齐输出）对于提升LRMs事实性的关键作用。">
                           MR-ALIGN: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models
                       </div>
                       <div class="publication-authors">Xinming Wang, Jian Xu, Bin Yu, Sheng Lian, Hongzhu Yi, <span class="highlight">Yi Chen</span>, Boran Wang, Haoran Du, Han Hu, Xuyao Zhang, Chenglin Liu</div>
                       <div class="publication-links">
                            <a href="#"><i class="fas fa-file-pdf"></i> arXiv (Coming Soon)</a>
                       </div>
                   </li>

                   <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="One Patch Doesn’t Fit All: Adaptive Patching for Native-Resolution Multimodal Large Language Models"
                             data-lang-zh="One Patch Doesn’t Fit All：面向原生分辨率多模态大模型的自适应Patching策略"
                             data-abstract-en="Real-world visual signals are inherently variable in resolution, and it is natural to endow multimodal large language models (MLLMs) with such native-resolution perception capabilities. In principle, for general and straightforward multimodal understanding, low-resolution images are sufficient. While for images with nuanced details like documents and charts, it is crucial to preserve fine-grained details using high-resolution inputs, as naive resizing inevitably results in information loss. Recent advances employ sequence packing to process images of any resolution and aspect ratios. Despite these efforts, model performance degrades at both low and high resolutions, and high-resolution inputs incur substantial computational costs. We argue that the rigid use of a single patch size is the primary cause: when image resolution or information density varies, fixing patch size is intrinsically suboptimal.  To address this issue, we introduce Adaptive Patching (AdaPatch), a simple yet effective strategy that adjusts patch size according to image resolution and information density and could be seamlessly plugged into pre-trained fixed-patch MLLMs without any training efforts. Extensive evaluations demonstrate consistent improvements in native resolution performance without additional training. Besides, we provide a training-based method to further adapt MLLMs with dynamic patch sizes and enhance the performance."
                             data-abstract-zh="现实世界的视觉信号在分辨率上具有内在的多变性，赋予多模态大语言模型（MLLMs）这种原生分辨率感知能力是很自然的。原则上，对于通用且直接的多模态理解，低分辨率图像已经足够。然而，对于包含细微细节的图像（如文档和图表），使用高分辨率输入来保留细粒度细节至关重要，因为简单的缩放不可避免地导致信息丢失。最近的进展采用序列打包来处理任意分辨率和长宽比的图像。尽管有这些努力，模型性能在低分辨率和高分辨率下都会下降，且高分辨率输入会产生巨大的计算成本。我们认为，僵化地使用单一尺寸的Patch是主要原因：当图像分辨率或信息密度变化时，固定Patch大小本质上是次优的。为解决这一问题，我们引入了自适应Patching（AdaPatch），这是一种简单而有效的策略，能够根据图像分辨率和信息密度调整Patch大小，并可无缝集成到预训练的固定Patch MLLMs中，无需任何训练工作。广泛的评估表明，在不进行额外训练的情况下，原生分辨率性能得到了一致提升。此外，我们提供了一种基于训练的方法，进一步使MLLMs适应动态Patch大小并增强性能。">
                           One Patch Doesn’t Fit All: Adaptive Patching for Native-Resolution Multimodal Large Language Models
                       </div>
                       <div class="publication-authors">Wenzhuo Liu, Weijie Yin, Fei Zhu, Shijie Ma, Haiyang Guo, <span class="highlight">Yi Chen</span>, Xiao-Hui Li, LiangXiao, ChaoFeng, Cheng-Lin Liu</div>
                       <div class="publication-links">
                            <a href="#"><i class="fas fa-file-pdf"></i> arXiv (Coming Soon)</a>
                       </div>
                   </li>

                   <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="The Evolution of Video Anomaly Detection: A Unified Framework from DNN to MLLM"
                             data-lang-zh="视频异常检测的演进：从DNN到MLLM的统一框架"
                             data-abstract-en="Video anomaly detection (VAD) aims to identify and ground anomalous behaviors or events in videos, serving as a core technology in the fields of intelligent surveillance and public safety. With the advancement of deep learning, the continuous evolution of deep model architectures has driven innovation in VAD methodologies, significantly enhancing feature representation and scene adaptability, thereby improving algorithm generalization and expanding application boundaries. More importantly, the rapid development of multi-modal large language (MLLMs) and large language models (LLMs) has introduced new opportunities and challenges to the VAD field. Under the support of MLLMs and LLMs, VAD has undergone significant transformations in terms of data annotation, input modalities, model architectures, and task objectives. The surge in publications and the evolution of tasks have created an urgent need for systematic reviews of recent advancements. This paper presents the first comprehensive survey analyzing VAD methods based on MLLMs and LLMs, providing an in-depth discussion of the changes occurring in the VAD field in the era of large models and their underlying causes. Additionally, this paper proposes a unified framework that encompasses both deep neural network (DNN)-based and LLM-based VAD methods, offering a thorough analysis of the new VAD paradigms empowered by LLMs, constructing a classification system, and comparing their strengths and weaknesses. Building on this foundation, this paper focuses on current VAD methods based on MLLMs/LLMs. Finally, based on the trajectory of technological advancements and existing bottlenecks, this paper distills key challenges and outlines future research directions, offering guidance for the VAD community."
                             data-abstract-zh="视频异常检测（VAD）旨在识别和定位视频中的异常行为或事件，是智能监控和公共安全领域的核心技术。随着深度学习的进步，深度模型架构的不断演变推动了VAD方法的创新，显著增强了特征表示和场景适应性，从而提高了算法的泛化能力并扩展了应用边界。更重要的是，多模态大语言模型（MLLMs）和大语言模型（LLMs）的快速发展为VAD领域带来了新的机遇和挑战。在MLLMs和LLMs的支持下，VAD在数据标注、输入模态、模型架构和任务目标方面经历了重大变革。出版物的激增和任务的演变使得对近期进展进行系统回顾的需求日益迫切。本文首次全面综述了基于MLLMs和LLMs的VAD方法，深入探讨了大模型时代VAD领域发生的变革及其根本原因。此外，本文提出了一个涵盖基于深度神经网络（DNN）和基于LLM的VAD方法的统一框架，彻底分析了LLMs赋能的新VAD范式，构建了分类体系，并比较了它们的优缺点。在此基础上，本文重点关注当前基于MLLMs/LLMs的VAD方法。最后，基于技术进步的轨迹和现有的瓶颈，本文提炼了关键挑战并概述了未来的研究方向，为VAD社区提供指导。">
                           The Evolution of Video Anomaly Detection: A Unified Framework from DNN to MLLM
                       </div>
                       <div class="publication-authors">Shibo Gao, Peipei Yang, Haiyang Guo, Yangyang Liu, <span class="highlight">Yi Chen</span>, Shuai Li, Han Zhu, Jian Xu, Xu-Yao Zhang, Linlin Huang</div>
                       <div class="publication-links">
                           <a href="https://arxiv.org/abs/2507.21649"><i class="fas fa-file-pdf"></i> arXiv</a>
                       </div>
                   </li>

                   <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction"
                             data-lang-zh="MeteorPred：面向极端天气预测的气象多模态大模型与数据集"
                             data-abstract-en="Timely and accurate severe weather alerts are critical for disaster mitigation. However, current forecasting systems rely largely on expert knowledge, resulting in subjectivity and high labor demands. With the rapid development of AI technologies, the end-to-end AI weather station is gradually emerging as a new trend in predicting severe weather events. Three core challenges impede the development of end-to-end AI severe weather system: (1) scarcity of severe weather event samples; (2) imperfect alignment between high-dimensional meteorological data and textual warnings; (3) existing multimodal language models are unable to handle high-dimensional meteorological data and struggle to fully capture the complex dependencies across temporal sequences, vertical pressure levels, and spatial dimensions. To address these challenges, we introduce MP-Bench, the first large-scale temporal multimodal dataset for severe weather events prediction, comprising 421,363 pairs of raw multi-year meteorological data and corresponding  text caption, covering a wide range of severe weather scenarios across China. On top of this dataset, we develop a meteorology multimodal large model (MMLM) that directly ingests 4D meteorological inputs. In addition, it is designed to accommodate the unique characteristics of 4D meteorological data flow, incorporating three plug-and-play adaptive fusion modules that enable dynamic feature extraction and integration across modalities, temporal sequences, vertical pressure layers, and spatial dimensions. Extensive experiments on MP-Bench demonstrate that MMLM performs exceptionally well across multiple tasks, highlighting its effectiveness in severe weather understanding and marking a key step toward realizing automated, AI-driven weather forecasting systems."
                             data-abstract-zh="及时且准确的极端天气预警对于减灾至关重要。然而，当前的预报系统在很大程度上依赖专家知识，导致主观性和高人力需求。随着AI技术的快速发展，端到端AI气象站正逐渐成为预测极端天气事件的新趋势。三个核心挑战阻碍了端到端AI极端天气系统的发展：（1）极端天气事件样本稀缺；（2）高维气象数据与文本预警之间的对齐不完善；（3）现有的多模态语言模型无法处理高维气象数据，且难以完全捕捉跨时间序列、垂直气压层和空间维度的复杂依赖关系。为解决这些挑战，我们推出了MP-Bench，这是首个用于极端天气事件预测的大规模时序多模态数据集，包含421,363对原始多年气象数据和相应的文本描述，覆盖了中国各地的广泛极端天气场景。在此基础上，我们开发了一种气象多模态大模型（MMLM），能够直接处理4D气象输入数据。此外，它旨在适应4D气象数据流的独特特性，融合了三个即插即用的自适应融合模块，实现了跨模态、时间序列、垂直气压层和空间维度的动态特征提取与整合。在MP-Bench上的广泛实验表明，MMLM在多项任务中表现出色，突显了其在极端天气理解方面的有效性，标志着向实现自动化、AI驱动的天气预报系统迈出了关键一步。">
                           MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction
                       </div>
                       <div class="publication-authors">Shuo Tang, Jian Xu, Jiadong Zhang, <span class="highlight">Yi Chen</span>, Qizhao Jin, Lingdong Shen, Cheng-Lin Liu, Shiming Xiang </div>
                       <div class="publication-links">
                           <a href="https://arxiv.org/abs/2508.06859"><i class="fas fa-file-pdf"></i> arXiv</a>
                       </div>
                   </li>

                   <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="The Hitchhiker's Guide to Scientific Agents: A Journey Through the Cosmos of Research Automation"
                             data-lang-zh="科学智能体银河指南：探索科研自动化之旅"
                             data-abstract-en="The advancement of LLM-based agents heralds a new perspective for AI for Science (AI4S), automation science research. Prominent large language models, such as DeepSeek-R1 and OpenAI-o1, have exhibited expertise across multiple domains, prompting researchers to develop agents for the natural sciences and investigate the frontiers of scientific knowledge. Nevertheless, the divergences between the natural sciences and AI have hindered the development and advancement of scientific agents across various fields. This survey is grounded in the standardized scientific research process and elucidates the construction and evaluation of scientific agents. Initially, we delineate the substantial distinctions between scientific agents and general-purpose agents with regard to goal orientation, and presents a systematic taxonomy based on construction strategies and capability scopes. Subsequently, we examine the fundamental procedure for constructing scientific agents from inception and the strategy for targeted capability enhancement within a dual-layer progressive framework. Additionally, we outline the benchmarking and evaluation approaches for scientific agents at different stages of science research. Ultimately, we investigate prospective research directions for scientific agents. It is our aspiration that this survey will assist researchers across diverse research fields in constructing specialized scientific agents, while fostering innovation in the realm of scientific agents to further AI-driven automation science research."
                             data-abstract-zh="基于大语言模型的智能体进展为AI for Science (AI4S) 和自动化科学研究带来了新的视角。DeepSeek-R1和OpenAI-o1等杰出的大语言模型已在多个领域展示了专业知识，促使研究人员开发用于自然科学的智能体并探索科学知识的前沿。然而，自然科学与AI之间的差异阻碍了各个领域科学智能体的开发与进步。本综述立足于标准化的科学研究流程，阐述了科学智能体的构建与评估。首先，我们界定了科学智能体与通用智能体在目标导向方面的实质性区别，并提出了基于构建策略和能力范围的系统分类体系。随后，我们考察了从零开始构建科学智能体的基本程序，以及双层递进框架内针对性能力增强的策略。此外，我们概述了科学研究不同阶段科学智能体的基准测试和评估方法。最后，我们探讨了科学智能体的未来研究方向。我们期望本综述能协助不同研究领域的研究人员构建专门的科学智能体，同时促进科学智能体领域的创新，以进一步推动AI驱动的自动化科学研究。">
                           The Hitchhiker's Guide to Scientific Agents: A Journey Through the Cosmos of Research Automation
                       </div>
                       <div class="publication-authors">Xinming Wang, Aslan Feng, Jian Xu, <span class="highlight">Yi Chen</span>, Haiyang Guo, Fei Zhu, Minsi Ren, Yuanqi Shao, Hongzhu Yi, Hongming Yang, Winston Hu, Tailin Wu, Xuyao Zhang, Cheng-Lin Liu</div>
                       <div class="publication-links">
                           <a href="https://www.techrxiv.org/users/951553/articles/1320864-the-hitchhiker-s-guide-to-autonomous-research-a-survey-of-scientific-agents"><i class="fas fa-file-pdf"></i> TechRxiv</a>
                           <a href="https://github.com/gudehhh666/Awesome_Scientific_Agent"><i class="fab fa-github"></i> GitHub</a>
                       </div>
                   </li>
                   
                   <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="Fine-Grained Post-Training Quantization for Large Vision Language Models with Integrated Gradients"
                             data-lang-zh="基于积分梯度的多模态大模型细粒度后训练量化方法"
                             data-abstract-en="Large Vision Language Models (LVLMs) have made breakthrough in various downstream areas that requires multi-modal interaction. However, the powerful capabilities come with a surge in computational overhead and memory usage, which hinders the practical deployment. Among numerous acceleration techniques, quantization is a popular strategy to effectively reduce memory cost and accelerate inference. Despite the great progress in LVLM quantization, exiting methods mainly measure the sensitivity of input tokens based on modality, which fails to express the rich and complicated information between inputs, highlighting the need to quantize from a fine-grained and interpretable way. Drawn from the concept of axiomatic attribution from mechanistic interpretability, we exploit integrated gradients to effectively evaluate the sensitivity, and push the granularity from modality to token to reflect both inter-modality and intra-modality dynamics. We conduct comprehensive experiments under W4A8 and W3A16 quantization for various LVLMs, and the results show that only a 6% increase in quantization time brings substantial performance improvements, demonstrating the superiority of our method."
                             data-abstract-zh="大型视觉语言模型（LVLMs）在需要多模态交互的各个下游领域取得了突破。然而，强大的能力伴随着激增的计算开销和内存使用，阻碍了实际部署。在众多加速技术中，量化是一种有效降低内存成本并加速推理的流行策略。尽管LVLM量化取得了巨大进展，但现有方法主要基于模态来衡量输入Token的敏感度，无法表达输入之间丰富且复杂的信息，突显了以细粒度和可解释方式进行量化的必要性。借鉴机理可解释性中的公理归因概念，我们利用积分梯度有效评估敏感度，并将粒度从模态推向Token级别，以反映模态间和模态内的动态变化。我们在W4A8和W3A16量化下对各种LVLMs进行了全面实验，结果表明，仅增加6%的量化时间即可带来显著的性能提升，证明了我们方法的优越性。">
                           Fine-Grained Post-Training Quantization for Large Vision Language Models with Integrated Gradients
                       </div>
                       <div class="publication-authors">Ziwen Xiang, Fanhu Zeng, Hongjian Fang, Rui-Qi Wang, Renxing Chen, <span class="highlight">Yi Chen</span>, Yanan Zhu, Peipei Yang, Xu-Yao Zhang</div>
                       <div class="publication-links">
                           <a href="https://arxiv.org/abs/2507.21649"><i class="fas fa-file-pdf"></i> arXiv</a>
                       </div>
                   </li>
                </ul>
            </div>
            
            <div class="card">
                <h2 class="section-title">
                    <i class="fas fa-book-journal-whills"></i>
                    <span class="lang-switch" data-lang-en="Peer-Reviewed Publications" data-lang-zh="同行评审论文">Peer-Reviewed Publications</span>
                </h2>
                <ul class="publication-list">
                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="ManiNet: Manifold Network for Few-Shot Learning"
                             data-lang-zh="ManiNet：面向小样本学习的流形网络"
                             data-abstract-en="Few-shot Learning (FSL) aims to learn a model that can be seamlessly adapted to unknown classes with only a few labeled data. A concise but successful way is to learn a robust feature encoder to describe novel classes relying on given supervised data. Under the guidance of such insight, most methods define classes as standard Gaussian distributions with different means in feature spaces, where classification can be performed based on the distances between embedding and class centroids. In spite of considerable achievements, these methods always miss the structural information within classes, resulting in degraded performance. To tackle this problem, we develop a novel yet concise approach named Manifold Network (ManiNet) to perform few-shot classification based on manifolds. Technically, in the ManiNet, each class is represented as a tree rather than an isolated centroids to reserve the structural information. And a simple correction term is introduced to elevate the usage of data by representing each manifold with a graph. Benefiting from such modeling, the probability of unknown data belonging to a class is derived based on the relative energy change before and after adding this data into the class manifolds. Experimental results on popular benchmarks strongly demonstrate that our ManiNet suffices to achieve competitive performance with simpler modeling and higher robustness, compared to the previous state-of-the-art."
                             data-abstract-zh="小样本学习（FSL）旨在仅利用少量标注数据即可无缝适应未知类别的模型。一种简洁而成功的方法是学习一个鲁棒的特征编码器，依靠给定的监督数据来描述新颖类别。在此见解的指导下，大多数方法将类别定义为特征空间中具有不同均值的标准高斯分布，其中分类可以根据嵌入与类别质心之间的距离进行。尽管取得了相当大的成就，但这些方法总是忽略类别内的结构信息，导致性能下降。为解决这一问题，我们开发了一种新颖而简洁的方法——流形网络（ManiNet），基于流形进行小样本分类。在技术上，ManiNet将每个类别表示为树结构而非孤立的质心，以保留结构信息。并通过引入一个简单的修正项，通过用图表示每个流形来提升数据的使用率。受益于这种建模，未知数据属于某个类别的概率是根据将该数据加入类别流形前后的相对能量变化推导出来的。在流行基准上的实验结果有力地证明，与之前的最先进技术相比，我们的ManiNet能够以更简单的建模和更高的鲁棒性实现具有竞争力的性能。">
                            ManiNet: Manifold Network for Few-Shot Learning
                        </div>
                        <div class="publication-authors">Ruiqi Wang, Hengcan Shi, <span class="highlight">Yi Chen</span>, YaoNan Wang</div>
                        <div class="publication-venue">AIHCIR 2025 (Best Paper Award)</div>
                        <div class="publication-links">
                            <a href="#"><i class="fas fa-file-pdf"></i> PDF</a>
                        </div>
                    </li>

                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly Grounding and Understanding"
                             data-lang-zh="VAGU & GtS：基于大语言模型的视频异常定位与理解联合基准与框架"
                             data-abstract-en="Video Anomaly Detection (VAD) aims to identify anomalous events within videos and precisely ground their temporal occurrences. Current VAD research can be primarily categorized into two paradigms: traditional DNN-based methods predominantly focus on temporal grounding of anomalies, while emerging methods leveraging Large Language Models(LLMs) emphasize the semantic understanding of anomalous content. In fact, both video anomaly understanding and video anomaly grounding tasks are crucial for the comprehensive detection of anomalies in videos. Furthermore, these two tasks can mutually support and corroborate each other. However, there is currently no model capable of simultaneously accomplishing both tasks, nor is there a dataset that can support both tasks. In this paper, we propose VAGU (Video Anomaly Grounding and Understanding), the first benchmark integrating both anomaly grounding and anomaly understanding. Specifically, each instance in VAGU is annotated with three manually curated components: anomaly category specification, anomaly semantic understanding, and precise anomaly temporal grounding. Building upon this benchmark, we present Glance then Scrutinize (GtS) - a training-free framework guided by static textual and dynamic textual prompts. This framework achieves coarse grounding of high-probability anomalous regions without requiring anomaly-specific prompts, followed by fine-grained anomaly interpretation and temporal boundary refinement. Furthermore, we innovatively propose the Joint evaluation of Anomaly Understanding and Grounding (JeAUG) metric, which overcomes the unidimensional limitations of conventional evaluation systems by jointly assessing semantic interpretability accuracy and temporal grounding precision. Extensive experiments demonstrate the superiority of our proposed benchmark, framework, and evaluation metrics."
                             data-abstract-zh="视频异常检测（VAD）旨在识别视频中的异常事件并精确定位其发生时间。当前的VAD研究主要可分为两种范式：传统的基于DNN的方法主要关注异常的时间定位，而利用大语言模型（LLMs）的新兴方法则强调对异常内容的语义理解。事实上，视频异常理解和视频异常定位任务对于视频中异常的全面检测都至关重要。此外，这两个任务可以相互支持和印证。然而，目前没有任何模型能够同时完成这两个任务，也没有能够支持这两个任务的数据集。在本文中，我们提出了VAGU（视频异常定位与理解），这是首个整合了异常定位和异常理解的基准。具体而言，VAGU中的每个实例都标注了三个由人工整理的组件：异常类别说明、异常语义理解和精确的异常时间定位。在此基准之上，我们提出了Glance then Scrutinize (GtS) —— 一个由静态和动态文本提示引导的免训练框架。该框架无需特定异常提示即可实现高概率异常区域的粗略定位，随后进行细粒度的异常解释和时间边界细化。此外，我们创新性地提出了异常理解和定位联合评估（JeAUG）指标，通过联合评估语义可解释性准确性和时间定位精度，克服了传统评估系统的一维局限性。广泛的实验证明了我们提出的基准、框架和评估指标的优越性。">
                            VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly Grounding and Understanding
                        </div>
                        <div class="publication-authors">Shibo Gao, Peipei Yang, <span class="highlight">Yi Chen</span>, Han Zhu, Yangyang Liu, Wenxin Zhang, Linlin Huang</div>
                        <div class="publication-venue">AAAI 2026</div>
                        <div class="publication-links">
                            <a href="https://arxiv.org/abs/2507.21507"><i class="fas fa-file-pdf"></i> arXiv</a>
                        </div>
                    </li>

                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information"
                             data-lang-zh="可恢复压缩：文本信息引导的多模态视觉Token恢复机制"
                             data-abstract-en="With the advancement of large-scale language modeling techniques, large multimodal models combining visual encoders with large language models have demonstrated exceptional performance in various visual tasks. Most of the current large multimodal models achieve this by mapping visual features obtained from the visual encoder into a large language model and using them as inputs alongside text for downstream tasks. Therefore, the number of visual tokens directly affects the training and inference speed of the model. There has been significant work on token pruning for visual transformers, but for large multimodal models, only relying on visual information for token pruning or compression may lead to significant loss of important information. On the other hand, the textual input in the form of a question may contain valuable information that can aid in answering the question, providing additional knowledge to the model. To address the potential oversimplification and excessive pruning that can occur with most purely visual token pruning methods, we propose a text information-guided dynamic visual token recovery mechanism that does not require training. This mechanism leverages the similarity between the question text and visual tokens to recover visually meaningful tokens with important text information while merging other less important tokens, to achieve efficient computation for large multimodal models. Experimental results demonstrate that our proposed method achieves comparable performance to the original approach while compressing the visual tokens to an average of 10% of the original quantity."
                             data-abstract-zh="随着大规模语言建模技术的进步，结合视觉编码器与大语言模型的大型多模态模型在各种视觉任务中展现出卓越性能。当前大多数大型多模态模型通过将视觉编码器提取的视觉特征映射到大语言模型中，并与文本一同作为下游任务的输入来实现这一目标。因此，视觉标记的数量直接影响模型的训练和推理速度。针对视觉变换器的标记剪枝已有大量研究工作，但对于大型多模态模型，仅依赖视觉信息进行标记剪枝或压缩可能导致重要信息的显著丢失。另一方面，以问题形式输入的文本可能包含有助于回答问题的宝贵信息，为模型提供额外知识。为解决大多数纯视觉标记剪枝方法可能存在的过度简化与过度剪枝问题，我们提出了一种无需训练的文本信息引导的动态视觉标记恢复机制。该机制利用问题文本与视觉标记之间的相似性，恢复具有重要文本信息的视觉意义标记，同时合并其他次要标记，以实现大型多模态模型的高效计算。实验结果表明，所提方法在将视觉标记压缩至原始数量平均10%的同时，取得了与原始方法相当的性能。">
                            Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information
                        </div>
                        <div class="publication-authors"><span class="highlight">Yi Chen</span>, Jian Xu, Xu-Yao Zhang, Wen-Zhuo Liu, Yang-Yang Liu, Cheng-Lin Liu</div>
                        <div class="publication-venue">AAAI 2025</div>
                        <div class="publication-links">
                            <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32229"><i class="fas fa-globe"></i> Link</a>
                            <a href="https://arxiv.org/abs/2409.01179"><i class="fas fa-file-pdf"></i> arXiv</a>
                            <a href="https://github.com/banjiuyufen/Recoverable-Compression"><i class="fab fa-github"></i> Code</a>
                        </div>
                    </li>

                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="Decoupling Layout from Glyph in Online Chinese Handwriting Generation"
                             data-lang-zh="联机中文手写生成中的字形与布局解耦"
                             data-abstract-en="Text plays a crucial role in the transmission of human civilization, and teaching machines to generate online handwritten text in various styles presents an interesting and significant challenge. However, most prior work has concentrated on generating individual Chinese fonts, leaving complete text line generation largely unexplored. In this paper, we identify that text lines can naturally be divided into two components: layout and glyphs. Based on this division, we designed a text line layout generator coupled with a diffusion-based stylized font synthesizer to address this challenge hierarchically. More concretely, the layout generator performs in-context-like learning based on the text content and the provided style references to generate positions for each glyph autoregressively. Meanwhile, the font synthesizer which consists of a character embedding dictionary, a multi-scale calligraphy style encoder and a 1D U-Net based diffusion denoiser will generate each font on its position while imitating the calligraphy style extracted from the given style references. Qualitative and quantitative experiments on the CASIA-OLHWDB demonstrate that our method is capable of generating structurally correct and indistinguishable imitation samples."
                             data-abstract-zh="文字在人类文明的传承中起着至关重要的作用，教机器生成各种风格的联机手写文本是一个有趣且意义重大的挑战。然而，大多数先前的工作都集中在生成单个中文字体上，而完整的文本行生成在很大程度上未被探索。在本文中，我们发现文本行可以自然地分为两个部分：布局和字形。基于这一划分，我们设计了一个文本行布局生成器，并结合一个基于扩散模型的风格化字体合成器，以分层的方式解决这一挑战。具体而言，布局生成器基于文本内容和提供的风格参考进行类上下文学习，自回归地生成每个字形的位置。同时，由字符嵌入字典、多尺度书法风格编码器和基于1D U-Net的扩散去噪器组成的字体合成器，将在其位置生成每个字体，同时模仿从给定风格参考中提取的书法风格。在CASIA-OLHWDB上的定性和定量实验表明，我们的方法能够生成结构正确且难以区分的模仿样本。">
                            Decoupling Layout from Glyph in Online Chinese Handwriting Generation
                        </div>
                        <div class="publication-authors">Min-Si Ren, Yan-Ming Zhang, <span class="highlight">Yi Chen</span></div>
                        <div class="publication-venue">ICLR 2025</div>
                        <div class="publication-links">
                            <a href="https://arxiv.org/abs/2410.02309"><i class="fas fa-file-pdf"></i> arXiv</a>
                            <a href="https://github.com/singularityrms/OLHWG"><i class="fab fa-github"></i> Code</a>
                        </div>
                    </li>

                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="Recognition of Online Handwritten Chinese Texts in Any Writing Direction via Stroke Classification Based Over-Segmentation"
                             data-lang-zh="基于笔画分类过切分的任意书写方向联机中文手写文本识别"
                             data-abstract-en="Online handwritten text recognition technology has been increasingly applied in intelligent touch-based and pen-based devices. Current mainstream methods are mostly designed for horizontally written texts, thus is difficult to handle texts in any writing direction. This paper proposes a recognition framework based on over-segmentation which is applicable to text recognition of any writing direction. It divides text line inclination styles into two cases: texts with the entire line rotated and texts with the line direction rotated while keeping the characters upright. A text line inclination style classification module is introduced in the preprocessing stage to classify these two cases. The former case can be recognized using a horizontal text line recognizer after rotation correction. For the latter case, an improved over-segmentation algorithm is designed based on stroke classification using bidirectional long short-term memory networks (BiLSTM) to achieve text recognition in any writing direction. Experimental results demonstrate that the proposed method is capable of text recognition in any writing direction and achieves highly competitive results on the CASIA-OLHWDB and ICDAR2013-Online datasets."
                             data-abstract-zh="联机手写文本识别技术已越来越多地应用于智能触摸和笔式设备中。目前的主流方法大多针对横向书写的文本设计，因此难以处理任意书写方向的文本。本文提出了一种基于过切分的识别框架，适用于任意书写方向的文本识别。它将文本行倾斜风格分为两种情况：整行旋转的文本和行方向旋转但保持字符直立的文本。在预处理阶段引入了文本行倾斜风格分类模块来分类这两种情况。前一种情况在旋转校正后可以使用水平文本行识别器进行识别。对于后一种情况，设计了一种基于双向长短时记忆网络（BiLSTM）进行笔画分类的改进过切分算法，以实现任意书写方向的文本识别。实验结果表明，该方法能够进行任意书写方向的文本识别，并在CASIA-OLHWDB和ICDAR2013-Online数据集上取得了极具竞争力的结果。">
                            Recognition of Online Handwritten Chinese Texts in Any Writing Direction via Stroke Classification Based Over-Segmentation
                        </div>
                        <div class="publication-authors"><span class="highlight">Yi Chen</span>, Heng Zhang, Min-Si Ren, Cheng-Lin Liu</div>
                        <div class="publication-venue">ICPR 2024</div>
                        <div class="publication-links">
                            <a href="https://link.springer.com/chapter/10.1007/978-3-031-78183-4_24"><i class="fas fa-globe"></i> Link</a>
                        </div>
                    </li>

                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="Context-Aware Confidence Estimation for Rejection in Handwritten Chinese Text Recognition"
                             data-lang-zh="手写中文文本识别中用于拒识的上下文感知置信度估计"
                             data-abstract-en="Handwritten Chinese Text Recognition (HCTR) has been advanced largely by deep learning in recent years. However, the remaining recognition errors still hinder reliability-critical applications where zero-error is desired. Rejecting low-confidence patterns can help reduce the error rate but the increased rejection rate is also harmful. In this paper, we propose a character confidence estimation method incorporating contexts for character rejection in HCTR. Based on a text line recognizer outputting character segmentation and classification results, the confidence of each segmented character is estimated by combining the scores of a re-trained character classifier, the linguistic and geometric contexts. We introduce a probabilistic formula for estimating the confidence by combining the classifier and contextual scores, and an improved approach for scoring the geometric context using unary and binary geometric features. Experimental evaluations on the CASIA-HWDB and ICDAR2013 datasets demonstrate that our method can significantly improve the rejection performance in respect of low error rate at moderate rejection rate. The re-trained classifier, the linguistic context and the geometric context are all justified effective to improve the confidence."
                             data-abstract-zh="近年来，深度学习极大地推动了手写中文文本识别（HCTR）的发展。然而，残留的识别错误仍然阻碍了需要零错误的关键可靠性应用。拒绝低置信度模式有助于降低错误率，但增加的拒绝率也是有害的。在本文中，我们提出了一种结合上下文的字符置信度估计方法，用于HCTR中的字符拒识。基于输出字符切分和分类结果的文本行识别器，通过结合重训练的字符分类器分数、语言上下文和几何上下文，来估计每个切分字符的置信度。我们引入了一个通过结合分类器和上下文分数来估计置信度的概率公式，以及一种利用一元和二元几何特征对几何上下文进行评分的改进方法。在CASIA-HWDB和ICDAR2013数据集上的实验评估表明，我们的方法可以在中等拒绝率下显著提高低错误率方面的拒识性能。重训练的分类器、语言上下文和几何上下文均被证明能有效提高置信度。">
                            Context-Aware Confidence Estimation for Rejection in Handwritten Chinese Text Recognition
                        </div>
                        <div class="publication-authors">Yang-Yang Liu, <span class="highlight">Yi Chen</span>, Fei Yin, Cheng-Lin Liu</div>
                        <div class="publication-venue">ICDAR 2024</div>
                        <div class="publication-links">
                            <a href="https://link.springer.com/chapter/10.1007/978-3-031-70533-5_9"><i class="fas fa-globe"></i> Link</a>
                        </div>
                    </li>

                    <li>
                        <div class="publication-title lang-switch"
                             data-lang-en="Improved Learning for Online Handwritten Chinese Text Recognition with Convolutional Prototype Network"
                             data-lang-zh="基于卷积原型网络的联机中文手写文本识别改进学习方法"
                             data-abstract-en="Segmentation-based handwritten text recognition has the advantage of character interpretability but needs a character classifier with high classification accuracy and non-character rejection capability. The classifier can be trained on both character samples and string samples but real string samples are usually insufficient. In this paper, we proposed a learning method for segmentation-based online handwritten Chinese text recognition with a convolutional prototype network as the underlying classifier. The prototype classifier is inherently resistant to non-characters, and so, can be trained with character and string samples without the need of data augmentation. The learning has two stages: pre-training on character samples with a modified loss function for improving non-character resistance, and weakly supervised learning on both character and string samples for improving recognition performance. Experimental results on the CASIA-OLHWDB and ICDAR2013-Online datasets show that the proposed method can achieve promising recognition performance without training data augmentation."
                             data-abstract-zh="基于切分的手写文本识别具有字符可解释性的优势，但需要一个具有高分类精度和非字符拒识能力的字符分类器。分类器可以在字符样本和字符串样本上进行训练，但真实的字符串样本通常不足。在本文中，我们提出了一种基于切分的联机中文手写文本识别学习方法，采用卷积原型网络作为底层分类器。原型分类器天生具有抗非字符干扰的能力，因此无需数据增强即可利用字符和字符串样本进行训练。学习过程分为两个阶段：使用改进的损失函数在字符样本上进行预训练以提高抗非字符干扰能力，以及在字符和字符串样本上进行弱监督学习以提高识别性能。在CASIA-OLHWDB和ICDAR2013-Online数据集上的实验结果表明，该方法无需训练数据增强即可实现令人满意的识别性能。">
                            Improved Learning for Online Handwritten Chinese Text Recognition with Convolutional Prototype Network
                        </div>
                        <div class="publication-authors"><span class="highlight">Yi Chen</span>, Heng Zhang, Cheng-Lin Liu</div>
                        <div class="publication-venue">ICDAR 2023</div>
                        <div class="publication-links">
                            <a href="https://link.springer.com/chapter/10.1007/978-3-031-41685-9_3"><i class="fas fa-globe"></i> Link</a>
                        </div>
                    </li>
                </ul>
            </div>
        </div>
        
        <div class="footer">
            <p>© 2025 Yi Chen</p>
            <p>State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS)</p>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // --- Global State ---
            // Try to load saved language, default to 'en'
            let currentLang = localStorage.getItem('lang') || 'en';
            
            // --- Tooltip Logic ---
            const tooltip = document.getElementById('abstract-tooltip');
            const tooltipContent = document.getElementById('tooltip-content');
            let hoverTimer;

            // Function to update tooltip text based on current language
            function updateTooltipText(element) {
                const abstractEn = element.getAttribute('data-abstract-en');
                const abstractZh = element.getAttribute('data-abstract-zh');
                
                if (currentLang === 'zh' && abstractZh) {
                    tooltipContent.textContent = abstractZh;
                } else {
                    tooltipContent.textContent = abstractEn || "Abstract not available.";
                }
            }

            // Function to position tooltip
            function positionTooltip(e) {
                const tooltipWidth = 600; // Match CSS width
                const tooltipMargin = 15;
                
                let left = e.clientX + tooltipMargin;
                let top = e.clientY + tooltipMargin;
                
                // Get viewport dimensions
                const viewportWidth = window.innerWidth;
                const viewportHeight = window.innerHeight;
                
                // Check right boundary
                if (left + tooltipWidth > viewportWidth) {
                    left = e.clientX - tooltipWidth - tooltipMargin;
                }
                
                // Check bottom boundary (approximate tooltip height)
                const estimatedHeight = 250; // Increased estimate for long abstracts
                if (top + estimatedHeight > viewportHeight) {
                    // Flip to top if not enough space below
                    top = e.clientY - estimatedHeight;
                    // Ensure it doesn't go off top
                    if (top < 10) top = 10;
                }
                
                // Safety clamp
                if (left < 10) left = 10;

                tooltip.style.left = `${left}px`;
                tooltip.style.top = `${top}px`;
            }

            document.querySelectorAll('.publication-title').forEach(title => {
                title.addEventListener('mouseenter', function(e) {
                    // Update text immediately
                    updateTooltipText(this);
                    
                    // Position immediately
                    positionTooltip(e);
                    
                    // Show
                    clearTimeout(hoverTimer);
                    tooltip.classList.add('visible');
                });

                title.addEventListener('mousemove', function(e) {
                    // Follow mouse
                    positionTooltip(e);
                });

                title.addEventListener('mouseleave', function() {
                    // Hide
                    tooltip.classList.remove('visible');
                });
            });

            // --- Theme Toggle ---
            const themeToggle = document.getElementById('themeToggle');
            const themeIcon = themeToggle.querySelector('i');
            
            if (localStorage.getItem('theme') === 'dark') {
                document.documentElement.classList.add('dark-mode');
                themeIcon.className = 'fas fa-sun';
            }

            themeToggle.addEventListener('click', () => {
                document.documentElement.classList.toggle('dark-mode');
                const isDark = document.documentElement.classList.contains('dark-mode');
                themeIcon.className = isDark ? 'fas fa-sun' : 'fas fa-moon';
                localStorage.setItem('theme', isDark ? 'dark' : 'light');
            });

            // --- Language Toggle ---
            const langToggle = document.getElementById('langToggle');
            const langText = langToggle.querySelector('span');

            // Initialize Language on Load
            updateLanguageUI();

            function updateLanguageUI() {
                // Update button text
                langText.textContent = currentLang === 'en' ? 'EN' : '中';
                
                // Update all page text
                document.querySelectorAll('.lang-switch').forEach(el => {
                    const text = el.getAttribute(`data-lang-${currentLang}`);
                    if (text) el.innerHTML = text;
                });
            }

            langToggle.addEventListener('click', () => {
                currentLang = currentLang === 'en' ? 'zh' : 'en';
                localStorage.setItem('lang', currentLang);
                updateLanguageUI();
            });
        });
    </script>
</body>
</html>
